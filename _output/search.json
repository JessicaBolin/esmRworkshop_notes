[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for marine ecologists: wrangling Earth System Model outputs",
    "section": "",
    "text": "Preface\nWorkshop date: 05/03/2025\nLocation: Lecture Hall, Bodega Marine Laboratory\nInstructors: Jessica Bolin, Mikaela Provost, Nerea Lezama-Ochoa, Mer Pozo Buil, Mary Fisher\nR is undoubtedly the programming language of choice for many marine ecologists. However, navigating the complexities of downloading, processing and wrangling gridded oceanographic data within R can represent a steep learning curve. As climate-related projects continue to receive funding and the impacts of climate variability and change on our oceans become increasingly evident, the ability to efficiently interact with, and use, Earth System Model (ESM) outputs for our research is more critical than ever.\nThe aim of our one-day R course is to train students, researchers and faculty in skills needed to be proficient in creating projections of physical variables from ESM outputs, with the goal of using them for ecological applications. Specifically, we’ll be creating projections of sea-surface temperature across two time periods out to 2100, across two climate scenarios, for the California Current region.\nOur workshop is designed around Step 8 (pre-process the data) in the workflow of best practices for using ESMs for marine ecologists, as defined in Figure 1 within (Schoeman et al. 2023).\n\nOur workshop is supported by CMSI, and is hosted by Bodega Marine Lab. We thank both organisations for their funding and support.\n\n\n\n\nSchoeman, David S., Alex Sen Gupta, Cheryl S. Harrison, Jason D. Everett, Isaac Brito-Morales, Lee Hannah, Laurent Bopp, Patrick R. Roehrdanz, and Anthony J. Richardson. 2023. “Demystifying Global Climate Models for Use in the Life Sciences.” Trends in Ecology & Evolution 38 (9): 843–58. https://doi.org/https://doi.org/10.1016/j.tree.2023.04.005.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "_x_Acronyms.html",
    "href": "_x_Acronyms.html",
    "title": "Acronyms",
    "section": "",
    "text": "We will be using a lot of acronyms during today’s workshop..!\n\nACCESS-CM2 = Australian Community Climate and Earth System Simulator Coupled Model Version 2\nCDO = Climate Data Operators\nCMIP6 = Coupled Model Intercomparison Project Version 6\nESGF = Earth System Grid Federation\nESM = Earth System Model\nGCM = Global Coupled Model\nIPCC = Intergovernmental Panel on Climate Change\nIPSL-CM6A-LR = Institut Pierre-Simon Laplace Climate Model for Phase 6 of CMIP - Low Resolution\nnetCDF = Network Common Data Form (file format = .nc)\nOISST = Optimum Interpolation Sea Surface Temperature\nomon = monthly time step\nwget = World Wide Web Get\nSSPs = Shared socio-economic pathways\nSST = Sea-surface temperature\ntos = sea-surface temperature (temperature ocean surface)",
    "crumbs": [
      "Acronyms"
    ]
  },
  {
    "objectID": "_xx_about.html",
    "href": "_xx_about.html",
    "title": "About",
    "section": "",
    "text": "Instructors\nToday’s instructors are Jessica Bolin, Mikaela Provost, Mer Pozo Buil, Nerea Lezama-Ochoa and Mary Fisher\nJessie is a Postdoc in Provost Lab, working on modeling and mapping climate refugia for red abalone across California. Jessie has a background in quantitative marine science and obtained her PhD from the University of the Sunshine Coast in 2024, and has broad research interests spanning climate adaptation, fisheries forecasting, marine disease ecology and sustainable seafood. Jessie has been using R since 2016, and has worked with ESMs since 2021. jabbolin@ucdavis.edu\nMikaela is an Assistant Professor at UC Davis, and the leader of Provost Lab. Mikaela’s research uses theoretical and empirical approaches to address applied problems in fisheries management and marine conservation. Her projects span multiple species and places, and the methods I use can be adapted for a wide variety of taxa. mmprovost@ucdavis.edu\nMer is an Associate Project Scientist at the University of California Santa Cruz. Mer’s research interests are in ocean modeling and large-scale circulation, specifically about subsurface ocean variability, large-scale changes in water masses and circulation, and how they affect upwelling systems.\nNerea is an Associate Project Scientist at the University of California Santa Cruz. Nerea is currently working on the conservation and management of species taken incidentally in tropical tuna purse seine fisheries (bycatch). Nerea’s main research interests are ecological modeling, spatial ecology, oceanography and ecosystem-based management approaches for the conservation of marine top predators.\nMary is a Postdoc in the Baskett Lab, and an interdisciplinary marine scientist interested in how adaptation to climate change affects coupled human-natural systems. Her research has drawn on quantitative and qualitative techniques to explore trade-offs and unexpected consequences associated with climate adaptation in US West Coast fisheries.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#chapter-index",
    "href": "_xx_about.html#chapter-index",
    "title": "About",
    "section": "Chapter index",
    "text": "Chapter index\n\nWhat is an ESM?\n\nExperiments\nClimate scenarios (SSPs)\n\nDownload ESM output from ESGF\n\nThe ESGF database\nDownload wget shell scripts\nDownload .nc files with wget scripts\n\nRemapping\n\nInspect an ESM\nRemapping with CDO and R\n\nOISST / observed data\n\nDownload OISST\nPreprocess OISST\nVisualize OISST\n\nBias correction & downscaling\n\nOISST and ESM climatologies\nESM anomalies\nInterpolate ESM anomalies to OISST grid\nBias correction\n\nEvaluate accuracy of historical ESM projections\n\nBrief explainer\nCreate baseline climatologies\nTaylor Diagrams\n\nMaking projections\n\nTime series\nSpatial projections\nUncertainty",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#workshop-materials",
    "href": "_xx_about.html#workshop-materials",
    "title": "About",
    "section": "Workshop materials",
    "text": "Workshop materials\nThe workshop data are ~15 GB, so ensure you have enough storage space on your machine before downloading, or bring an external SSD/Hard-drive.\nIf you’re a Github user, send Jessie your Github username to be given access to the repo, and you will then be able to fork and clone the repo here. Then, you will then need to download the workshop __data folder here, which needs to be copied into the root directory of esmRworkshop_notes. We’ve added the /__data folder to the .gitignore file within the root directory, since we’ll be working with large files (and Github breaks when pushing/pulling large files).\nIf you don’t use Github, you can instead download the complete workshop repository here.\nAlternatively, you can obtain the materials on the day from us via USB.\nOn the day, we’ll be working with code contained in the __scripts directory.\nWe have inserted the expected time duration for running processor-intensive chunks of code in blockquotes throughout the eBook. This is based on Jessie’s machine (MacBook Pro 2023 M3 Max 64 GB Memory). Your machine may be faster or slower, depending on its specifications.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#what-to-bring",
    "href": "_xx_about.html#what-to-bring",
    "title": "About",
    "section": "What to bring",
    "text": "What to bring\n\nLaptop (highly, highly, highly recommend macOS) with internet connection. Please ensure you have administrator permissions - consult with your IT department on how to do this. Windows laptops are OK, but the instructors are all macOS natives - we will be using macOS during the workshop, and are unlikely to be able to fix any Windows-related software installation problems that are unfortunately common with some of the tools we’ll be using.\nInstall the necessary software, listed on the Prerequisites page.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#expected-outcomes",
    "href": "_xx_about.html#expected-outcomes",
    "title": "About",
    "section": "Expected outcomes",
    "text": "Expected outcomes\nBy the end of the course, participants should be able to:\n\nDownload, inspect, and wrangle netCDF file formats within R\nUse CDO to speed up netCDF file manipulation\nDownload ESM output from ESGF and understand terminology re. variables/naming conventions\nRemap and bias-correct ESM outputs from the CMIP6 suite across multiple climate scenarios and time periods\nMake publication-quality figures",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#transportation-tofrom-uc-davis",
    "href": "_xx_about.html#transportation-tofrom-uc-davis",
    "title": "About",
    "section": "Transportation to/from UC Davis",
    "text": "Transportation to/from UC Davis\nWe are arranging for a UCD minivan to transport participants to/from UC Davis main campus, The van has space for up to 12 participants - first-in, first-served. Sign up sheet TBA.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#food",
    "href": "_xx_about.html#food",
    "title": "About",
    "section": "Food",
    "text": "Food\nLunch (sandwiches) will be provided, with vegan and vegetarian options available.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#acknowledgements",
    "href": "_xx_about.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe acknowledge funding from the UC Davis Coastal and Marine Science Institute’s internal grant scheme (2024-2025), and thank Bodega Marine Laboratory for providing space and equipment to run the workshop.\nWe are very thankful for Barb Muhling (UCSC) and Alice Pidd (UniSC) who reviewed workshop materials. We also thank Isaac Brito-Morales for developing the following R workshop materials here and here on analysing netCDFs and climate data, which our workshop took inspiration from. We also thank Dave Schoeman who developed and shared the first iteration of some code in this workshop, and who introduced Jessie to the world of climate models in the first place!",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xx_about.html#about-this-book",
    "href": "_xx_about.html#about-this-book",
    "title": "About",
    "section": "About this book",
    "text": "About this book\nThis is a Quarto book. To learn more, click here.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "_xxx_prerequisites.html",
    "href": "_xxx_prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nOur course is not intended for absolute beginners in R. While participants don’t need to be highly proficient, a basic understanding of R and programming syntax is required. For example, participants should know how to create an R project and open a script, read in data using commands, make basic plots using base R, and write for loops. A basic understanding of the shell/command line is advantageous, but not required. Further, previous experience using the terra package and working with netCDF files is a plus, but again, not required.\nIn addition to having R and RStudio already installed, you will need to install (i) CDO, (ii) NCO, (iii) wget, (iv) Panoply and (v) a suite of R packages.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "_xxx_prerequisites.html#prerequisites",
    "href": "_xxx_prerequisites.html#prerequisites",
    "title": "Prerequisites",
    "section": "",
    "text": "Important\n\n\n\nWe strongly recommend bringing a macOS laptop to the workshop. We’ve found that Windows users often run into issues with installing and using CDO, which is required for the workshop (see below). If you have a Windows laptop, you’re still welcome to attend, but the instructors will be using macOS during the workshop. As such, we’re unlikely to be able to fix any Windows-related errors or software installation problems (and we unfortunately can’t spend time troubleshooting any Windows issues).\nTo see if you may have software issues during our workshop, install the prerequisites below, download this folder here, and run the runme.R script. If you don’t run into any errors, great! If you do, you will need to do some troubleshooting before the workshop.\n\n\n\n\nCDO\nYou will need CDO (Climate Data Operators) installed prior to the workshop. CDO comprises a suite of over 600 operators for standard (and speedy) processing of climate and forecast model outputs. More information on CDO found here.\n\nMacOS\nThe simplest way to install CDO is via Homebrew. If you haven’t already installed Homebrew, do so here. Then, open terminal and run the following code:\n\nbrew install cdo\n\nMore information available on the CDO MacOS website.\n\n\nWindows\nCDO is meant for use on POSIX-compatible operating systems (e.g., like Linux and MacOS), so downloading on Windows requires some extra steps.\nRecent versions of Windows (&gt;=10) includes an Ubuntu embedded Linux, offering the opportunity to install CDO via Ubuntu’s native package manager. First, install the Ubuntu app from the Microsoft Store application. Then open the Ubuntu terminal and type:\n\nsudo apt-get upgrade\nsudo apt-get install cdo #write your password, if prompted\n\nMore information available on the CDO Windows website.\n\n\n\n\nNCO\nSimilar to CDO, we will install NCO (netCDF operators), which are a suite of operators that take netCDF file formats and facilitate file manipulation.\n\nMac\n\nbrew install nco\n\n\n\nWindows\nInstalling NCO on windows can be tricky. Here is a discussion forum.\n\n\n\n\nwget\nwget is a bash command used for downloading files from the internet. We will use this to download our ESMs via wget scripts from the ESGF website.\n\nmacOS\nRun the following in terminal:\n\nbrew install wget\n\n\n\nWindows\nCheck out the link here. You may need to do some extra fiddling - see here or check out Youtube for further help.\n\n\n\n\nPanoply\nPanoply is a data viewer for netCDF, HDF and GRIB data arrays, administered by NASA Goddard Space Flight Center. Panoply requires that your computer has a compatible Java 11 (or later version) JRE or JDK installed.\nDownload Panoply here.\n\n\n\nR Packages\nEnsure you have the following R packages installed, listed below.\n\n#Install pacman \nif (!require(\"pacman\")) install.packages(\"pacman\") \n\n# Install packages, if not already installed \npacman::p_load(tidyverse, # Working with 'tidy' data\n               furrr, parallelly, purrr, # Run functions in parallel\n               tictoc, beepr, # Understand code execution time \n               ncdf4, raster, terra, # For working with netCDFs\n               tmap, # Map visualization \n               plotrix, # Taylor Diagrams\n               RCurl, xml2, rvest, # Downloading files \n               zoo) \n\n\n\n\nChanging your path\nLastly, once you have downloaded the repository, you will need to open __scripts/helpers.R, and change your pth object to suit your machine (see Line 7). I’ve set mine as \"/Users/admin/Documents/GitHub/esmRworkshop_website\"; change yours to reflect where this folder is located on your machine.\n\n\n\n\n\n\n\nBelow are the dependencies and package versions the instructors will use for the workshop.\n\n\n\n\n\n\n&gt; devtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.1 (2024-06-14)\n os       macOS Sonoma 14.5\n system   aarch64, darwin20\n ui       RStudio\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/Los_Angeles\n date     2025-05-28\n rstudio  2024.04.2+764 Chocolate Cosmos (desktop)\n pandoc   NA\n\n─ Packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n package      * version   date (UTC) lib source\n abind          1.4-5     2016-07-21 [1] CRAN (R 4.4.0)\n audio          0.1-11    2023-08-18 [1] CRAN (R 4.4.0)\n base64enc      0.1-3     2015-07-28 [1] CRAN (R 4.4.0)\n beepr        * 2.0       2024-07-06 [1] CRAN (R 4.4.0)\n bitops         1.0-8     2024-07-29 [1] CRAN (R 4.4.0)\n cachem         1.1.0     2024-05-16 [1] CRAN (R 4.4.0)\n class          7.3-22    2023-05-03 [1] CRAN (R 4.4.1)\n classInt       0.4-10    2023-09-05 [1] CRAN (R 4.4.0)\n cli            3.6.3     2024-06-21 [1] CRAN (R 4.4.0)\n codetools      0.2-20    2024-03-31 [1] CRAN (R 4.4.1)\n colorspace     2.1-0     2023-01-23 [1] CRAN (R 4.4.0)\n crosstalk      1.2.1     2023-11-23 [1] CRAN (R 4.4.0)\n DBI            1.2.3     2024-06-02 [1] CRAN (R 4.4.0)\n devtools       2.4.5     2022-10-11 [1] CRAN (R 4.4.0)\n dichromat      2.0-0.1   2022-05-02 [1] CRAN (R 4.4.0)\n digest         0.6.36    2024-06-23 [1] CRAN (R 4.4.0)\n dplyr        * 1.1.4     2023-11-17 [1] CRAN (R 4.4.0)\n e1071          1.7-14    2023-12-06 [1] CRAN (R 4.4.0)\n ellipsis       0.3.2     2021-04-29 [1] CRAN (R 4.4.0)\n fansi          1.0.6     2023-12-08 [1] CRAN (R 4.4.0)\n fastmap        1.2.0     2024-05-15 [1] CRAN (R 4.4.0)\n forcats      * 1.0.0     2023-01-29 [1] CRAN (R 4.4.0)\n fs             1.6.4     2024-04-25 [1] CRAN (R 4.4.0)\n furrr        * 0.3.1     2022-08-15 [1] CRAN (R 4.4.0)\n future       * 1.34.0    2024-07-29 [1] CRAN (R 4.4.0)\n generics       0.1.3     2022-07-05 [1] CRAN (R 4.4.0)\n ggplot2      * 3.5.1     2024-04-23 [1] CRAN (R 4.4.0)\n globals        0.16.3    2024-03-08 [1] CRAN (R 4.4.0)\n glue           1.7.0     2024-01-09 [1] CRAN (R 4.4.0)\n gtable         0.3.5     2024-04-22 [1] CRAN (R 4.4.0)\n hms            1.1.3     2023-03-21 [1] CRAN (R 4.4.0)\n htmltools      0.5.8.1   2024-04-04 [1] CRAN (R 4.4.0)\n htmlwidgets    1.6.4     2023-12-06 [1] CRAN (R 4.4.0)\n httpuv         1.6.15    2024-03-26 [1] CRAN (R 4.4.0)\n httr           1.4.7     2023-08-15 [1] CRAN (R 4.4.0)\n KernSmooth     2.23-24   2024-05-17 [1] CRAN (R 4.4.1)\n later          1.3.2     2023-12-06 [1] CRAN (R 4.4.0)\n lattice        0.22-6    2024-03-20 [1] CRAN (R 4.4.0)\n leafem         0.2.3     2023-09-17 [1] CRAN (R 4.4.0)\n leaflet        2.2.2     2024-03-26 [1] CRAN (R 4.4.0)\n leafsync       0.1.0     2019-03-05 [1] CRAN (R 4.4.0)\n lifecycle      1.0.4     2023-11-07 [1] CRAN (R 4.4.0)\n listenv        0.9.1     2024-01-29 [1] CRAN (R 4.4.0)\n lubridate    * 1.9.3     2023-09-27 [1] CRAN (R 4.4.0)\n lwgeom         0.2-14    2024-02-21 [1] CRAN (R 4.4.0)\n magrittr       2.0.3     2022-03-30 [1] CRAN (R 4.4.0)\n maps           3.4.2     2023-12-15 [1] CRAN (R 4.4.0)\n memoise        2.0.1     2021-11-26 [1] CRAN (R 4.4.0)\n mime           0.12      2021-09-28 [1] CRAN (R 4.4.0)\n miniUI         0.1.1.1   2018-05-18 [1] CRAN (R 4.4.0)\n munsell        0.5.1     2024-04-01 [1] CRAN (R 4.4.0)\n ncdf4        * 1.22      2023-11-28 [1] CRAN (R 4.4.0)\n parallelly   * 1.41.0    2024-12-18 [1] CRAN (R 4.4.1)\n pillar         1.9.0     2023-03-22 [1] CRAN (R 4.4.0)\n pkgbuild       1.4.4     2024-03-17 [1] CRAN (R 4.4.0)\n pkgconfig      2.0.3     2019-09-22 [1] CRAN (R 4.4.0)\n pkgload        1.4.0     2024-06-28 [1] CRAN (R 4.4.0)\n plotrix      * 3.8-4     2023-11-10 [1] CRAN (R 4.4.0)\n png            0.1-8     2022-11-29 [1] CRAN (R 4.4.0)\n profvis        0.3.8     2023-05-02 [1] CRAN (R 4.4.0)\n promises       1.3.0     2024-04-05 [1] CRAN (R 4.4.0)\n proxy          0.4-27    2022-06-09 [1] CRAN (R 4.4.0)\n purrr        * 1.0.2     2023-08-10 [1] CRAN (R 4.4.0)\n R6             2.5.1     2021-08-19 [1] CRAN (R 4.4.0)\n raster       * 3.6-30    2024-10-02 [1] CRAN (R 4.4.1)\n RColorBrewer   1.1-3     2022-04-03 [1] CRAN (R 4.4.0)\n Rcpp           1.0.12    2024-01-09 [1] CRAN (R 4.4.0)\n RCurl        * 1.98-1.16 2024-07-11 [1] CRAN (R 4.4.0)\n readr        * 2.1.5     2024-01-10 [1] CRAN (R 4.4.0)\n remotes        2.5.0     2024-03-17 [1] CRAN (R 4.4.0)\n rlang          1.1.4     2024-06-04 [1] CRAN (R 4.4.0)\n rstudioapi     0.16.0    2024-03-24 [1] CRAN (R 4.4.0)\n rvest        * 1.0.4     2024-02-12 [1] CRAN (R 4.4.0)\n scales         1.3.0     2023-11-28 [1] CRAN (R 4.4.0)\n sessioninfo    1.2.2     2021-12-06 [1] CRAN (R 4.4.0)\n sf             1.0-16    2024-03-24 [1] CRAN (R 4.4.0)\n shiny          1.8.1.1   2024-04-02 [1] CRAN (R 4.4.0)\n sp           * 2.1-4     2024-04-30 [1] CRAN (R 4.4.0)\n stars          0.6-6     2024-07-16 [1] CRAN (R 4.4.0)\n stringi        1.8.4     2024-05-06 [1] CRAN (R 4.4.0)\n stringr      * 1.5.1     2023-11-14 [1] CRAN (R 4.4.0)\n terra        * 1.8-29    2025-02-26 [1] CRAN (R 4.4.1)\n tibble       * 3.2.1     2023-03-20 [1] CRAN (R 4.4.0)\n tictoc       * 1.2.1     2024-03-18 [1] CRAN (R 4.4.0)\n tidyr        * 1.3.1     2024-01-24 [1] CRAN (R 4.4.0)\n tidyselect     1.2.1     2024-03-11 [1] CRAN (R 4.4.0)\n tidyverse    * 2.0.0     2023-02-22 [1] CRAN (R 4.4.0)\n timechange     0.3.0     2024-01-18 [1] CRAN (R 4.4.0)\n tmap         * 3.3-4     2023-09-12 [1] CRAN (R 4.4.0)\n tmaptools      3.1-1     2021-01-19 [1] CRAN (R 4.4.0)\n tzdb           0.4.0     2023-05-12 [1] CRAN (R 4.4.0)\n units          0.8-5     2023-11-28 [1] CRAN (R 4.4.0)\n urlchecker     1.0.1     2021-11-30 [1] CRAN (R 4.4.0)\n usethis        2.2.3     2024-02-19 [1] CRAN (R 4.4.0)\n utf8           1.2.4     2023-10-22 [1] CRAN (R 4.4.0)\n vctrs          0.6.5     2023-12-01 [1] CRAN (R 4.4.0)\n viridisLite    0.4.2     2023-05-02 [1] CRAN (R 4.4.0)\n withr          3.0.0     2024-01-16 [1] CRAN (R 4.4.0)\n XML            3.99-0.17 2024-06-25 [1] CRAN (R 4.4.0)\n xml2         * 1.3.6     2023-12-04 [1] CRAN (R 4.4.0)\n xtable         1.8-4     2019-04-21 [1] CRAN (R 4.4.0)\n zoo          * 1.8-12    2023-04-13 [1] CRAN (R 4.4.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "_xxxx_schedule.html",
    "href": "_xxxx_schedule.html",
    "title": "Schedule (TBD)",
    "section": "",
    "text": "TBD: Van departs UCD main campus to BML (sign-up required)\n1000: Introductions and housekeeping\n1010: ESM overview\n1030: Downloading ESMs from ESGF\n1130: Remapping ESMs\n1230-1330: Lunch\n1330: OISST\n1400: Bias-correction and downscaling\n1500: Evaluating ESM accuracy\n1530: Making projections\n1630-1700: Questions, troubleshooting, workshop wrap-up.\nTBD: Van departs BML to UCD main campus (sign-up required)",
    "crumbs": [
      "Schedule (TBD)"
    ]
  },
  {
    "objectID": "_1_whatisanesm.html",
    "href": "_1_whatisanesm.html",
    "title": "1  What is an ESM?",
    "section": "",
    "text": "1.1 Experiments\nModelers run different experiments to simulate past, present and future climates (e.g., what would happen if we suddenly quadrupled CO2, or, if CO2 gradually increased from pre-industrial levels by 1% per year). A full list of CMIP6 experiments can be found here.\nNot all modeling centers run the same experiments in their ESMs. However, each ESM contains the historical simulation, where the ESM is run over the historical period, defined as 1850-2014. The historical runs are not fit to observed data, but rather, emerge from the physics of the model. This allows modelers to run hindcasts of the model (i.e., predictions of past climate), which are then compared to recorded climate observations. If the hindcasts are good at simulating observed reality, this gives modelers more confidence in the model projections.\nAnother set of experiments are future warming scenarios, termed SSPs, which are detailed below.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is an ESM?</span>"
    ]
  },
  {
    "objectID": "_1_whatisanesm.html#climate-scenarios-ssps",
    "href": "_1_whatisanesm.html#climate-scenarios-ssps",
    "title": "1  What is an ESM?",
    "section": "1.2 Climate scenarios (SSPs)",
    "text": "1.2 Climate scenarios (SSPs)\nClimate scenarios represent plausible storylines of future changes in population, demographics, and energy use. In CMIP5, scenarios were expressed as representative concentration pathways (RCPs) which describe greenhouse gas emission pathways (offset by other emissions) that result in specified levels of radiative forcing in 2100.\nIn CMIP6, climate scenarios now incorporate narratives describing alternative shared socioeconomic pathways (SSPs), particularly relating to the Paris Agreement. The five most common SSPs in CMIP6 are listed below (sensu Table 2 and Figure 1 in (Schoeman et al. 2023)).\n\n\n\n\n\n\n\n\nScenario\nDescription\nWarming relative to preindustrial (90% confidence interval)\n\n\nSSP1-1.9\nNet zero CO2 emissions achieved by mid-century; avoids exceeding 1.5°C of warming, in line with the ambition of the Paris Agreement\nStabilises at 1.4°C (1.0–1.8°C), with minimal overshoot beyond 1.5 °C\n\n\nSSP1-2.6\nNet zero CO2 emissions achieved in the latter part of this century; achieves the goals of the Paris Agreement by avoiding 2°C of warming\nStabilises at 1.8°C (1.3–2.4°C)\n\n\nSSP2-4.5\nApproximates current climate policy, although this will change with commitments at each successive Conference of the Parties\n2.7°C (2.1–3.5°C) by 2100\n\n\nSSP3-7.0\nApproximates a situation under which no new climate policy is implemented, resulting in a doubling of CO2 by 2100\n3.6°C (2.8–4.6°C) by 2100\n\n\nSSP5-8.5\nAn extreme counterfactual scenario under which CO2 emissions double by mid-century and increase thereafter\n4.4°C (3.3–5.7°C) by 2100",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is an ESM?</span>"
    ]
  },
  {
    "objectID": "_1_whatisanesm.html#esms-were-using-today",
    "href": "_1_whatisanesm.html#esms-were-using-today",
    "title": "1  What is an ESM?",
    "section": "1.3 ESMs we’re using today!",
    "text": "1.3 ESMs we’re using today!\nToday, we are using ocean temperature output from two ESMs, that are part of the CMIP6 effort: ACCESS-CM2 (Bi et al. 2020) and IPSL-CM6A-LR (Boucher et al. 2020). We are using output for two climate scenarios: SSP2-4.5 and SSP5-8.5.\nBelow, we’ve detailed key sub-model components for those interested:\nACCESS-CM2 represents one of Australia’s contributions to CMIP6.\n- Atmosphere: UM10.6 GA7.1\n- Land surface: CABLE v2.5 (coupled to the UM)\n- Sea ice: CICE5.1.2\n- Ocean: MOM5\n- Numerical coupler: OASIS3-MCT\nIPSL-CM6A-LR represents one of France’s contributions to CMIP6.\n- Atmosphere: LMDZ6A-LR\n- Land surface: ORCHIDEE v2.0\n- Ocean: NEMO v3.6\n- Sea ice: NEMO-LIM\n- Ocean biogeochemistry: NEMO-PISCES-v2\n- Numerical coupler: OASIS3-MCT\n\n\n\n\nBi, Daohua, Martin Dix, Simon Marsland, Siobhan O’Farrell, Arnold Sullivan, Roger Bodman, Rachel Law, et al. 2020. “Configuration and Spin-up of ACCESS-CM2, the New Generation Australian Community Climate and Earth System Simulator Coupled Model.” Journal of Southern Hemisphere Earth Systems Science 70 (1): 225–51. https://doi.org/10.1071/ES19040.\n\n\nBoucher, Olivier, Jérôme Servonnat, Anna Lea Albright, Olivier Aumont, Yves Balkanski, Vladislav Bastrikov, Slimane Bekki, et al. 2020. “Presentation and Evaluation of the IPSL-CM6A-LR Climate Model.” Journal of Advances in Modeling Earth Systems 12 (7): e2019MS002010. https://doi.org/https://doi.org/10.1029/2019MS002010.\n\n\nSchoeman, David S., Alex Sen Gupta, Cheryl S. Harrison, Jason D. Everett, Isaac Brito-Morales, Lee Hannah, Laurent Bopp, Patrick R. Roehrdanz, and Anthony J. Richardson. 2023. “Demystifying Global Climate Models for Use in the Life Sciences.” Trends in Ecology & Evolution 38 (9): 843–58. https://doi.org/https://doi.org/10.1016/j.tree.2023.04.005.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is an ESM?</span>"
    ]
  },
  {
    "objectID": "_2_downloadesm.html",
    "href": "_2_downloadesm.html",
    "title": "2  Download ESM output from ESGF",
    "section": "",
    "text": "2.1 Downloading CMIP6 Earth System Models\nThe Earth System Grid Federation (ESGF) manages the database for handling climate science data, and supports the Coupled Model Intercomparison Project, which is currently in its sixth iteration (CMIP6).\nToday, we will be downloading our model outputs via the ESGF website. We will download the simulations for two models, with the following components:\nFor more information on what ESM components mean, see Table 1 in (Schoeman et al. 2023).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Download ESM output from ESGF</span>"
    ]
  },
  {
    "objectID": "_2_downloadesm.html#downloading-cmip6-earth-system-models",
    "href": "_2_downloadesm.html#downloading-cmip6-earth-system-models",
    "title": "2  Download ESM output from ESGF",
    "section": "",
    "text": "Component Name\nVariable Name\nDescription\n\n\n\n\nActivity ID\nScenarioMIP\nScenarioMIP\n\n\nSource ID\nACCESS-CM2, IPSL-CM6A-LR\nModel names\n\n\nExperiment ID\nSSP2-4.5, SSP5-8.5 historical\nClimate scenario (historical ranges 1850-2014; SSPs range 2015-2100).\n\n\nModel variant label\nr1i1p1f1\nModel variant indicating the realization, initilization method, physics processes, and forcing datasets used in the model simulations.\n\n\nFrequency\nmon\nMonthly time-step\n\n\nVariable\ntos\nSea-surface temperature",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Download ESM output from ESGF</span>"
    ]
  },
  {
    "objectID": "_2_downloadesm.html#download-wget-shell-scripts",
    "href": "_2_downloadesm.html#download-wget-shell-scripts",
    "title": "2  Download ESM output from ESGF",
    "section": "2.2 Download wget shell scripts",
    "text": "2.2 Download wget shell scripts\n\n2.2.1 Navigate to the Metagrid node\nThrough the website:\n\nClick here to open the website\nClick the nodes link to explore the different nodes available (i.e., different data servers)\nClick a Metagrid UI node (i.e., today, we’ll use LLNL Metagrid)\n\nYou are now on the main website to download CMIP6 output. Ensure CMIP6 is selected under the ‘Project’ heading on the top left, and filter by ‘Only Globus Transferable’. On the left, there are several filters to play with.\n\n\n2.2.2 Filtering ESM components\nFrom the left tab, we use the following filters to find the wget script for the ACCESS-CM2 historical simulation:\n\nClick &gt;General, then under Activity ID, click CMIP.\nClick &gt;Classifications, then under Variable ID, click tos. Under Frequency, click mon.\nClick &gt;Identifiers, then under Source ID, click ACCESS-CM2. Under Experiment ID, click historical.\nClick &gt;Labels, then under Variant Label, click r1i1p1f1 (this is the most common variant).\n\n\n\n\n\n\n\n\n2.2.3 Download wget shell script\nThen, download the shell script by clicking the Download icon next to the wget download option. This uses wget to download a shell script (i.e., indicated by the .sh file extension) that we will later use to download the file through R whilst interfacing with CDO and the command line.\n\n\n\n\n\nOnce downloaded, move the shell script into the /wget_scripts folder in your project directory.\n\n\n2.2.4 Rinse and repeat\nNow, repeat the process for the SSP2-4.5 and SSP5-8.5 scenarios for ACCESS-CM2, in addition to all three scenarios for IPSL-CM6A-LR. You will only need to change the (i) Activity ID from CMIP to ScenarioMIP, and (ii) Experiment ID filter from historical to ssp245 and ssp585, and You will end up with six shell scripts, three for each model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Download ESM output from ESGF</span>"
    ]
  },
  {
    "objectID": "_2_downloadesm.html#download-.nc-files-with-wget-scripts",
    "href": "_2_downloadesm.html#download-.nc-files-with-wget-scripts",
    "title": "2  Download ESM output from ESGF",
    "section": "2.3 Download .nc files with wget scripts",
    "text": "2.3 Download .nc files with wget scripts\n\n\n\n\n\n\nWarning: storage\n\n\n\n\n\nIn total, we are about to download eight .nc files that comprise 2.65 GB of data. These are just for two models and two scenarios + historical, at a monthly time-step. Storing ESM outputs can take a lot of space - for example, if you’re working with daily data across multiple models and climate scenarios, you’ll almost certainly need an external hard drive or other storage solution.\n\n\n\nWe will use wget to download our files from the ESGF Metagrid.\n\n2.3.1 Define dependencies\nFirst, we source our packages and set our directories for where our (i) wget scripts are stored, and (ii) where we will store the ESM outputs.\n\nsource(paste0(getwd(), \"/__scripts/helpers.R\"))\n\n\n\n2.3.2 Prepare for parallel processing\nWe are harnessing the power of parallel processing to download our ESM outputs. We use availableCores() from the parallelly package to tell us how many available cores we have on our machine. I recommend to use whatever the output is minus two, so that your operating system and other background processes can continue working normally. I have 16 cores on my machine, so I’ll use 14 workers.\n\nparallelly::availableCores() # Output says I have 16 cores on my machine\nw &lt;- 14  # Leave two cores for other background processes\n\n\n\n2.3.3 Function to download files\nNow, we write a function that uses the system() function to invoke the terminal from within R to run the wget script, which downloads the ESM file. First, we set our working directory to the full path where data will be stored, which we defined above as /data. Then, we run the system() function to run the wget script using bash via the terminal. Once the script has run and the file has downloaded, we set the working directory back to the root directory.\n\nwget_files &lt;- function(script) {\n  setwd(paste0(pth, cmip_pth)) #set directory to where data will be stored\n  system(paste0(\"bash \", script, \" -s\")) #run wget on the shell script \n  setwd(pth) #set directory back to home directory \n}\n\nHere, we list all wget scripts that we downloaded from ESGF.\n\nfiles &lt;- list.files(paste0(pth, wget_pth), #full path where wget scripts are stored\n                    pattern = \"wget\", #only files with wget in the name\n                    full.names = TRUE #list the full path \n                    ) \nfiles\n\n[1] \"/Users/admin/Desktop/esmRworkshop_notes/__scripts/wget_scripts/wget_script_access-cm2_historical.sh\"\n[2] \"/Users/admin/Desktop/esmRworkshop_notes/__scripts/wget_scripts/wget_script_access-cm2_ssp245.sh\"    \n[3] \"/Users/admin/Desktop/esmRworkshop_notes/__scripts/wget_scripts/wget_script_access-cm2_ssp585.sh\"    \n[4] \"/Users/admin/Desktop/esmRworkshop_notes/__scripts/wget_scripts/wget_script_ipsl_historical.sh\"      \n[5] \"/Users/admin/Desktop/esmRworkshop_notes/__scripts/wget_scripts/wget_script_ipsl_ssp245.sh\"          \n[6] \"/Users/admin/Desktop/esmRworkshop_notes/__scripts/wget_scripts/wget_script_ipsl_ssp585.sh\"          \n\n\n\n\n2.3.4 Run function in parallel\nNow, we change to multi-session processing, where multiple workers (sessions) are used to download the files concurrently. We then run the wget_files() function with future_walk() , and use the tictoc package to time how long this takes. Once all files have downloaded, we change back to single-threaded processing.\n\nplan(multisession, workers = w) # Change to multi-threaded processing\ntic(); future_walk(files, wget_files); toc() #Run the function in parallel\nplan(sequential) # Return to single threaded processing (i.e., sequential/normal)\n\n\nJessie speed: 30s to 1 min\n\n\n\n\n\n\n\nDownload speeds\n\n\n\n\n\nDownload speeds depend on your Wifi connection, the capabilities of your local machine, and the performance of the server/node you’re attempting to connect to. You can check whether ESGF servers are online here.\n\n\n\nWe can check the files have downloaded by…\n\nlist.files(paste0(pth, cmip_pth))\n\n[1] \"tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc\"  \n[2] \"tos_Omon_ACCESS-CM2_ssp245_r1i1p1f1_gn_201501-210012.nc\"      \n[3] \"tos_Omon_ACCESS-CM2_ssp585_r1i1p1f1_gn_201501-210012.nc\"      \n[4] \"tos_Omon_IPSL-CM6A-LR_historical_r1i1p1f1_gn_185001-201412.nc\"\n[5] \"tos_Omon_IPSL-CM6A-LR_ssp245_r1i1p1f1_gn_201501-210012.nc\"    \n[6] \"tos_Omon_IPSL-CM6A-LR_ssp585_r1i1p1f1_gn_201501-210012.nc\"    \n\n\nGreat! We have successfully downloaded our ESM outputs.\nIf your scripts have downloaded data for 2100-2300 (i.e., some models will do this), we can remove those files with the rm argument in shell:\n\nsystem(paste0(\"rm \", pth, cmip_pth, \"/\", \n              \"tos_Omon_ACCESS-CM2_ssp585_r1i1p1f1_gn_210101-230012.nc\"))\nsystem(paste0(\"rm \", pth, cmip_pth, \"/\", \n              \"tos_Omon_IPSL-CM6A-LR_ssp585_r1i1p1f1_gn_210101-230012.nc\"))\n\n\n\n\n\n\n\nBeware of rm\n\n\n\n\n\nrm is a handy function for removing files, but there is no way of retrieving them if you’ve accidentally removed the wrong file. So, use this function wisely and make sure you’re 100% confident you’re removing the correct file!\n\n\n\n\n\n\n\nSchoeman, David S., Alex Sen Gupta, Cheryl S. Harrison, Jason D. Everett, Isaac Brito-Morales, Lee Hannah, Laurent Bopp, Patrick R. Roehrdanz, and Anthony J. Richardson. 2023. “Demystifying Global Climate Models for Use in the Life Sciences.” Trends in Ecology & Evolution 38 (9): 843–58. https://doi.org/https://doi.org/10.1016/j.tree.2023.04.005.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Download ESM output from ESGF</span>"
    ]
  },
  {
    "objectID": "_3_regridreproject.html",
    "href": "_3_regridreproject.html",
    "title": "3  Remapping",
    "section": "",
    "text": "3.1 Inspect an ESM",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remapping</span>"
    ]
  },
  {
    "objectID": "_3_regridreproject.html#inspect-an-esm",
    "href": "_3_regridreproject.html#inspect-an-esm",
    "title": "3  Remapping",
    "section": "",
    "text": "3.1.1 Panoply\nWe can quickly visualize what our ESM outputs look like, using the Panoply software. Panoply is a great tool to quickly inspect netCDF files, without reading them into R.\nOpen one of the .nc files in Panoply. You’ll see the dashboard below. The left pane shows the variables and dimensions contained within the file. The right pane shows the metadata in netCDF format.\n\n\n\n\n\nTo view a map, click on the variable tos, and then Create Plot in the top left hand corner. A new dialogue box pops up - accept the defaults and click Create.\n\n\n\n\n\nNow you can see a 2D field of sea-surface temperature for your time period of interest, and Panoply has georeferenced it. You can cycle between months in the Plot Controls dialogue box. If you can’t see this, click Window -&gt; Plot Controls from the Toolbar. Now, you can zoom in and move the plot around at will.\n\n\n\n\n\n\n\n3.1.2 R\nPanoply is great for quickly visualizing .nc files, but we can’t do any spatial analyses - we now need R. Let’s plot a layer of one of the ESMs we have downloaded, to see what it looks like.\n\nrr &lt;- rast(paste0(pth, cmip_pth, \"/tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc\")) \nrr &lt;- rr[[1]] #the first layer\nplot(rr)\n\nWarning: [is.lonlat] coordinates are out of range for lon/lat\n\n\n\n\n\n\n\n\n\nEw! What’s going on here!?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remapping</span>"
    ]
  },
  {
    "objectID": "_3_regridreproject.html#remapping",
    "href": "_3_regridreproject.html#remapping",
    "title": "3  Remapping",
    "section": "3.2 Remapping",
    "text": "3.2 Remapping\nDifferent institutions can serve their ESM’s on different grids. For example, many ESM grids are georeferenced on a sphere, where pole singularities and convergence of longitude meridians at the poles present issues for data visualisation. Panoply took care of this internally, but in R, we need to remap our files from a spherical grid to a rectangular grid. The most common remapping ‘method’ between grids with spherical coordinates is with bilinear interpolation.\n\n\n\n\n\n\nCaution\n\n\n\nYou can also use first order conservative remapping if bilinear interpolation fails (which may occur if the input grid is ‘unstructured’ as opposed to ‘spherical’).\n\n\nFirst, let’s inspect the metadata associated with our source input file using terra\n\nrr\n\nclass       : SpatRaster \ndimensions  : 300, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -0.5, 359.5, -0.5, 299.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +datum=WGS84 +no_defs \nsource      : tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc:tos \nvarname     : tos (Sea Surface Temperature) \nname        : tos_1 \nunit        :  degC \ntime (days) : 1850-01-16 \n\n\n\n\n\n\n\n\nWe can also check more detailed metadata contained in the ESM file using ncdf4::nc_open()\n\n\n\n\n\nThis gives you the same information after reading a file into Panoply.\n\nnc &lt;- nc_open(paste0(pth, cmip_pth, \"/tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc\"))\nnc\n\nFile /Users/admin/Desktop/esmRworkshop_notes/__data/cmip6_raw/tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc (NC_FORMAT_NETCDF4_CLASSIC):\n\n     6 variables (excluding dimension variables):\n        double time_bnds[bnds,time]   (Chunking: [2,1])  (Compression: level 1)\n        double latitude[i,j]   (Chunking: [360,300])  (Compression: level 1)\n            standard_name: latitude\n            long_name: latitude\n            units: degrees_north\n            missing_value: 1e+20\n            _FillValue: 1e+20\n            bounds: vertices_latitude\n        double longitude[i,j]   (Chunking: [360,300])  (Compression: level 1)\n            standard_name: longitude\n            long_name: longitude\n            units: degrees_east\n            missing_value: 1e+20\n            _FillValue: 1e+20\n            bounds: vertices_longitude\n        double vertices_latitude[vertices,i,j]   (Chunking: [2,360,300])  (Compression: level 1)\n            units: degrees_north\n            missing_value: 1e+20\n            _FillValue: 1e+20\n        double vertices_longitude[vertices,i,j]   (Chunking: [2,360,300])  (Compression: level 1)\n            units: degrees_east\n            missing_value: 1e+20\n            _FillValue: 1e+20\n        float tos[i,j,time]   (Chunking: [360,300,1])  (Compression: level 1)\n            standard_name: sea_surface_temperature\n            long_name: Sea Surface Temperature\n            comment: Temperature of upper boundary of the liquid ocean, including temperatures below sea-ice and floating ice shelves.\n            units: degC\n            cell_methods: area: mean where sea time: mean\n            cell_measures: area: areacello\n            history: 2019-11-08T18:45:39Z altered by CMOR: replaced missing value flag (-1e+20) with standard missing value (1e+20).\n            missing_value: 1.00000002004088e+20\n            _FillValue: 1.00000002004088e+20\n            coordinates: latitude longitude\n\n     5 dimensions:\n        time  Size:1980   *** is unlimited *** \n            bounds: time_bnds\n            units: days since 1850-01-01\n            calendar: proleptic_gregorian\n            axis: T\n            long_name: time\n            standard_name: time\n        j  Size:300 \n            units: 1\n            long_name: cell index along second dimension\n        i  Size:360 \n            units: 1\n            long_name: cell index along first dimension\n        bnds  Size:2 (no dimvar)\n        vertices  Size:4 (no dimvar)\n\n    47 global attributes:\n        Conventions: CF-1.7 CMIP-6.2\n        activity_id: CMIP\n        branch_method: standard\n        branch_time_in_child: 0\n        branch_time_in_parent: 0\n        creation_date: 2019-11-08T18:45:44Z\n        data_specs_version: 01.00.30\n        experiment: all-forcing simulation of the recent past\n        experiment_id: historical\n        external_variables: areacello\n        forcing_index: 1\n        frequency: mon\n        further_info_url: https://furtherinfo.es-doc.org/CMIP6.CSIRO-ARCCSS.ACCESS-CM2.historical.none.r1i1p1f1\n        grid: native atmosphere N96 grid (144x192 latxlon)\n        grid_label: gn\n        history: 2019-11-08T18:45:44Z ; CMOR rewrote data to be consistent with CMIP6, CF-1.7 CMIP-6.2 and CF standards.\n        initialization_index: 1\n        institution: CSIRO (Commonwealth Scientific and Industrial Research Organisation, Aspendale, Victoria 3195, Australia), ARCCSS (Australian Research Council Centre of Excellence for Climate System Science)\n        institution_id: CSIRO-ARCCSS\n        mip_era: CMIP6\n        nominal_resolution: 250 km\n        notes: Exp: CM2-historical; Local ID: bj594; Variable: tos (['sst'])\n        parent_activity_id: CMIP\n        parent_experiment_id: piControl\n        parent_mip_era: CMIP6\n        parent_source_id: ACCESS-CM2\n        parent_time_units: days since 0950-01-01\n        parent_variant_label: r1i1p1f1\n        physics_index: 1\n        product: model-output\n        realization_index: 1\n        realm: ocean\n        run_variant: forcing: GHG, Oz, SA, Sl, Vl, BC, OC, (GHG = CO2, N2O, CH4, CFC11, CFC12, CFC113, HCFC22, HFC125, HFC134a)\n        source: ACCESS-CM2 (2019): \naerosol: UKCA-GLOMAP-mode\natmos: MetUM-HadGEM3-GA7.1 (N96; 192 x 144 longitude/latitude; 85 levels; top level 85 km)\natmosChem: none\nland: CABLE2.5\nlandIce: none\nocean: ACCESS-OM2 (GFDL-MOM5, tripolar primarily 1deg; 360 x 300 longitude/latitude; 50 levels; top grid cell 0-10 m)\nocnBgchem: none\nseaIce: CICE5.1.2 (same grid as ocean)\n        source_id: ACCESS-CM2\n        source_type: AOGCM\n        sub_experiment: none\n        sub_experiment_id: none\n        table_id: Omon\n        table_info: Creation Date:(30 April 2019) MD5:e14f55f257cceafb2523e41244962371\n        title: ACCESS-CM2 output prepared for CMIP6\n        variable_id: tos\n        variant_label: r1i1p1f1\n        version: v20191108\n        cmor_version: 3.4.0\n        tracking_id: hdl:21.14100/0bcaaa74-aedb-4d45-a5e5-cb3ab467f2b5\n        license: CMIP6 model data produced by CSIRO is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/).  Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment.  Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file).  The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.\n\n\n\n\n\nWe can see that the resolution of our file is 1˚, and is on a 0-360˚ lat/lon grid. Let’s remap the file so it conforms to -180 to 180˚ lat/lon, crops the extent to the ocean off California, and changes the resolution to 0.25˚ to match the resolution of an observational data product we’ll be using in the next step (i.e., OISST for bias correction). Very important to note here that we’re not ‘increasing’ the resolution of our product, per se - we’re not gaining any data here, we’re just remapping the ESM to a finer resolution.\n\n\n3.2.1 A (brief) intro to CDO\nCDO provides a wide range of functions and operators for processing and analyzing climate data in netCDF format. CDO syntax is quite simple. CDO is often deployed through the terminal/shell. A basic line of code looks like:\ncdo -operator input_file/s output_file/s\nWhere we: (i) call cdo, (ii) specify the operator, (iii) define the input file/s, (iv) define the output file/s.\n\n\n\n\n\n\nCommon CDO operators\n\n\n\n\n\ncdo -yearmean calculates the annual mean of a monthly data input netCDF file\ncdo -yearmin calculates the annual min of a monthly data input netCDF file\ncdo -yearmax calculates the annual max of a monthly data input netCDF file\ncdo -ensmean calculates the ensemble mean of several netCDF files. If your input files are different models, this function will estimate a mean of all those models\ncdo -vertmean calculates the vertical mean for netCDF with olevel (i.e., depth)\ncdo -mergetime merge all the netCDF files in your directory\n\n\n\n\n\n3.2.2 Remapping with CDO and R\nHere, we define a function that leverages the power of CDO from within our R environment (as opposed through the terminal/shell).\n\nremap_n_crop_temp &lt;- function(nc_file,\n                              cell_res = 0.25, \n                              infold = paste0(pth, cmip_pth), \n                              outfold = paste0(pth, cmip_pth_proc),\n                              xmin = -126, xmax = -115, ymin = 32, ymax = 43) {\n  \n  system(paste0(\"cdo -L -sellonlatbox,\", xmin, \",\", xmax, \",\", ymin, \",\", ymax,  \n                \" -remapbil,r\", 360*(1/cell_res), \"x\", 180*(1/cell_res), \n                \" -select,name=tos \", infold, \"/\", nc_file, \" \", outfold, \"/\", nc_file))  \n  \n}\n\nfileys &lt;- list.files(paste0(pth, cmip_pth)) #list file names of downloaded ESMs\n\nw &lt;- 14 #number of workers\nplan(multisession, workers = w) # Change to multi-threaded processing\ntic(); future_walk(fileys, remap_n_crop_temp); toc() #Run the function in parallel (takes 8 sec for Jessie)\nplan(sequential) # Return to single threaded processing \n\n\nJessie speed: ~8s\n\n\n\n3.2.3 Function explainer\nFirst, the remap_n_crop_temp() function arguments:\n\nnc_file = the ESM file we downloaded from ESGF\ncell_res = the target resolution of your ESM, in degrees ˚\ninfold = the input folder where the ESM .nc files are stored (i.e., /data)\noutfold = the output folder where our processed ESM files will be stored (i.e., /processed_data/)\nxmin, xmax, ymin, ymax = the bounding box (lat/lon), of our extent encompassing the ocean off of California\n\nThe next chunk of code contained within the system() function uses CDO commands:\n\npaste0(\"cdo -L = Lock I/O (input/output read/write data sequential access)\nsellonlatbox,\" = select lon/lat box\nxmin, \",\", xmax, \",\", ymin, \",\", ymax, = boundaries of extent\n-remapbil,r\" = bilinear interpolation of the input grid (input grid MUST be curvilinear quadrilateral/spherical coordinates). Otherwise, use conservative remapping for an unstructured grid with -remapcon\n360*(1/cell_res), \"x\", 180*(1/cell_res) = adjust resolution according to rows (y) and columns (x) expected for target resolution.\n\" -select,name=tos \", = select variable name field from infile and write to outfile\ninfold, \"/\", nc_file, \" \", = input file path and file name\noutfold, \"/\", nc_file)) = output file path and file name\n\n\n\n\n\n\n\nImportant\n\n\n\nCDO requires syntax (i..e, spaces and commas) to be precise. If your code doesn’t run, first check that you have them in the right places.\n\n\nRunning this chunk of code within system() sends this code to the terminal/shell, opposed to R.\nThen, we list the names of all of our input files with fileys &lt;- list.files(\"data\").\nFinally, we use a for loop to iterate our function over each input file. We use the tic() and toc() functions from tictoc() to time how long our code takes to run. In my case, this takes ~30 seconds across the six files. R will outputs the following message after each iteration: # cdo(1) remapbil: Bilinear weights from curvilinear (360x300) to lonlat (1440x720) grid, with source mask (69782) - this is normal!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remapping</span>"
    ]
  },
  {
    "objectID": "_3_regridreproject.html#check-that-it-worked",
    "href": "_3_regridreproject.html#check-that-it-worked",
    "title": "3  Remapping",
    "section": "3.3 Check that it worked",
    "text": "3.3 Check that it worked\n\n# Read in one of the processed output files\nrr &lt;- rast(paste0(pth, cmip_pth_proc, \"/tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc\"))\nrr &lt;- rr[[1]] #first layer \nplot(rr, main = \"Remapped and cropped\")\nmaps::map(\"state\", add = T) #add US state boundaries\n\n\n\n\n\n\n\nrr #display metadata\n\nclass       : SpatRaster \ndimensions  : 44, 45, 1  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -126.125, -114.875, 32, 43  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc:tos \nvarname     : tos (Sea Surface Temperature) \nname        : tos_1 \nunit        :  degC \ntime (days) : 1850-01-16 \n\n\nGreat! We’ve successfully remapped to a rectangular lat/lon grid, changed the resolution to 0.25˚, and cropped the extent from global to just off California.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Remapping</span>"
    ]
  },
  {
    "objectID": "_4_oisst.html",
    "href": "_4_oisst.html",
    "title": "4  OISST",
    "section": "",
    "text": "4.1 Download OISST\nHere we define a function that takes two arguments, which denote the start and end of the time period we will use to define our baseline climatology. First, we will download twenty years worth of OISST data, spanning 1995 to 2014, which are our observations. We’ll get into why we chose this time period in the next Chapter. The function downloads daily files of OISST from the index site here.\nCode\n# Set destination folder\noFold &lt;- paste0(pth, \"/__data/oisst_raw/\")\n# Set base URL\nurl &lt;- \"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2.1/access/avhrr\"\n\n# Get the links that appear there as yyyymm dates\npg &lt;- read_html(url) # Read the HTML\nsFld &lt;- html_attr(html_nodes(pg, \"a\"), \"href\") %&gt;%  # Extract the links\n  grep(\"\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\", ., value = TRUE) # Just the folders with 6 digits (i.e., one per month)\n\n# Check what files might already be in the folder\nf &lt;- dir(oFold, pattern = \".nc\")\n\n# Extract identifiers from the file names - works at level of month\nncs &lt;- gsub(\"oisst-avhrr-v02r01.\", \"\", f) %&gt;%\n  gsub(\"\\\\d\\\\d\\\\.nc\", \"\", .) %&gt;% \n  unique() %&gt;% \n  paste0(., \"/\")\n\n# Exclude files (months) that are already downlaoded\nsFld &lt;- sFld[!sFld %in% ncs]\n\ndownload_oisst &lt;- function(yrstrt, yrend) {\n  \n  # Subset sFld to only include time period of interest\n  ind1 &lt;- grep(yrstrt, sFld) %&gt;% min \n  ind2 &lt;- grep(yrend, sFld) %&gt;% max\n  sFld &lt;- sFld[ind1:ind2] \n  length(sFld) #downloading 372 files for 30 yrs\n  \n  for(i in sFld) {\n    urli &lt;- paste0(url, \"/\", i)\n    pgi &lt;- read_html(urli) # Read the HTML\n    sFldi &lt;- html_attr(html_nodes(pgi, \"a\"), \"href\") %&gt;%  # Extract the links\n      grep(\".nc\", ., value = TRUE) # Just the netCDFs\n    for(j in sFldi) {\n      download.file(paste0(urli, j), paste0(oFold, j))\n    }\n  }\n}\n\ntic(); download_oisst(yrstrt = 1995, yrend = 2014); toc()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OISST</span>"
    ]
  },
  {
    "objectID": "_4_oisst.html#download-oisst",
    "href": "_4_oisst.html#download-oisst",
    "title": "4  OISST",
    "section": "",
    "text": "Jessie speed: ~2 hours (for 20 years of data).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OISST</span>"
    ]
  },
  {
    "objectID": "_4_oisst.html#preprocess-oisst",
    "href": "_4_oisst.html#preprocess-oisst",
    "title": "4  OISST",
    "section": "4.2 Preprocess OISST",
    "text": "4.2 Preprocess OISST\nAs with the ESMs, we need to preprocess them. However, OISST are provided as daily files, so we need to calculate monthly averages so they are comparable with our projections. We do this using the -mergetime and monmean operators. Then, as with the ESMs, we need to crop and remap the OISST fields to the same resolution and extent and off the California coast.\n\n\nCode\noisst_mr &lt;- function(yr,\n                     infile = paste0(pth, oisst_pth),\n                     outfile = paste0(pth, oisst_pth_proc),\n                     xmin = -126, xmax = -116, ymin = 32, ymax = 43,\n                     cell_res = 0.25) {\n  \n  \n  # Combine all daily files for X year into one file\n  merged_1yr &lt;- paste0(\"cdo -mergetime \", \n                 infile, \"/\", \"oisst-avhrr-v02r01.\", yr, \"*.nc \", \n                 outfile, \"/\", \"oisst-avhrr-merged_\", yr, \".nc\")\n  system(merged_1yr) # takes a few seconds\n  \n  \n  # Calculate monthly means for X year\n  mthmeans &lt;- paste0(\"cdo -L monmean \", \n                   outfile, \"/oisst-avhrr-merged_\", yr, \".nc \",\n                   outfile, \"/mean_\", yr, \".nc\")\n  system(mthmeans)\n  \n  # Select SST, crop and remap \n  oisst_regrid &lt;- paste0(\"cdo -L -select,name=,sst \", \n                   \"-sellonlatbox,\", xmin, \",\", xmax, \",\", ymin, \",\", ymax,  \n                   \" -remapbil,r\", 360*(1/cell_res), \"x\", 180*(1/cell_res), \n                   \" \", outfile, \"/mean_\", yr, \".nc\", \n                   \" \", outfile, \"/mean_remap_\", yr, \".nc\")\n  system(oisst_regrid)\n  \n  # Remove temporary files \n  system(paste0(\"rm \", outfile, \"/\", \"oisst-avhrr-merged_\", yr, \".nc\"))\n  system(paste0(\"rm \", outfile, \"/mean_\", yr, \".nc\"))\n  \n}\n\n# Vector of years to process\nyears &lt;- 1995:2014  # Modify as needed\n\nplan(multisession, workers = 14)  # Change workers to suit your machine\ntic(); future_map(years, ~ oisst_mr(.x)); toc()\nplan(sequential) # Return to sequential processing\n\n\n\nJessie speed: ~1-2 min\n\nOnce done, we will have a directory containing 20 years’ worth of preprocessed OISST monthly fields.\n\nlist.files(paste0(pth, \"/__data/oisst_processed\"))\n\n [1] \"mean_remap_1994.nc\" \"mean_remap_1995.nc\" \"mean_remap_1996.nc\"\n [4] \"mean_remap_1997.nc\" \"mean_remap_1998.nc\" \"mean_remap_1999.nc\"\n [7] \"mean_remap_2000.nc\" \"mean_remap_2001.nc\" \"mean_remap_2002.nc\"\n[10] \"mean_remap_2003.nc\" \"mean_remap_2004.nc\" \"mean_remap_2005.nc\"\n[13] \"mean_remap_2006.nc\" \"mean_remap_2007.nc\" \"mean_remap_2008.nc\"\n[16] \"mean_remap_2009.nc\" \"mean_remap_2010.nc\" \"mean_remap_2011.nc\"\n[19] \"mean_remap_2012.nc\" \"mean_remap_2013.nc\" \"mean_remap_2014.nc\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OISST</span>"
    ]
  },
  {
    "objectID": "_4_oisst.html#visualise",
    "href": "_4_oisst.html#visualise",
    "title": "4  OISST",
    "section": "4.3 Visualise",
    "text": "4.3 Visualise\nLet’s see how OISST compares to the ESM output.\n\nCode\noisst &lt;- terra::rast(paste0(pth, oisst_pth_proc, \"/mean_remap_1995.nc\"))[[1]]\nesm &lt;- terra::rast(paste0(pth, cmip_pth_proc, \"/tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc\"))[[1]]\nterra::plot(oisst, main = \"OISST Jan 1995\")\nmaps::map(\"world\", add = T)\nterra::plot(esm, main = \"ESM Jan 1850\")\nmaps::map(\"world\", add = T)\n\n\n\n\n\n\n\n\n\n\n\nNice! You can see that this data product, being at a finer resolution, resolves the coast much better than the ESMs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OISST</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html",
    "href": "_5_biascorrection.html",
    "title": "5  Bias correction & downscaling",
    "section": "",
    "text": "5.1 Merge daily OISST files\nMerge all daily OISST files into a single file\nCode\n# List all processed OISST files\nday_ncs &lt;- dir(paste0(pth, oisst_pth_proc), full.names = TRUE) %&gt;% \n  paste0(., collapse = \" \") \n\n# CDO code\ncdo_code &lt;- paste0(\"cdo -s -L \", #Deploy CDO silently and in low memory mode\n                   \"-f nc4 \",  # Output file should be netCDF format\n                   \"-z zip \", # Compress the output file using zip\n                   \"-mergetime \", # Merge multiple input netCDF files\n                   day_ncs, # The names of the input files\n                   \" \", \n                   paste0(pth, bc_pth, \"/_1_OISST_baseline_combined.nc\")) # output file \n\n# Run \ntic(); system(cdo_code); toc()\nThe .nc output has 240 layers (one for each month from January 1995 to December 2014).\nrr &lt;- terra::rast(paste0(pth, bc_pth, \"/_1_OISST_baseline_combined.nc\"))\nrr\n\nclass       : SpatRaster \ndimensions  : 44, 45, 252  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -126.125, -114.875, 32, 43  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : _1_OISST_baseline_combined.nc:sst \nvarname     : sst (Daily sea surface temperature) \nnames       : sst_zlev=0_1, sst_zlev=0_2, sst_zlev=0_3, sst_zlev=0_4, sst_zlev=0_5, sst_zlev=0_6, ... \nunit        :      Celsius,      Celsius,      Celsius,      Celsius,      Celsius,      Celsius, ... \ntime (days) : 1994-01-16 to 2014-12-16",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#merge-daily-oisst-files",
    "href": "_5_biascorrection.html#merge-daily-oisst-files",
    "title": "5  Bias correction & downscaling",
    "section": "",
    "text": "Jessie speed: 0.2s",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#create-oisst-climatology-1995-2014",
    "href": "_5_biascorrection.html#create-oisst-climatology-1995-2014",
    "title": "5  Bias correction & downscaling",
    "section": "5.2 Create OISST climatology (1995-2014)",
    "text": "5.2 Create OISST climatology (1995-2014)\n\n\n\n\n\n\nDefining ‘climatologies’\n\n\n\n\n\nHere, for simplicity, we’ve defined our ‘climatology’ as the SST averaged over the 20 year period between 1995-2014. However, you may want to consider calculating monthly climatologies (i.e., average SST over Jan, Feb, March etc. across the 20 year period), or even seasonal climatologies, so that you can capture and remove this sort of variability during the step of bias correction, for example. We highly recommend consulting with a physical oceanographer if doing bias correction and have questions about defining climatologies and temporal modes of variability.\n\n\n\n\n\nCode\ninput_file &lt;- paste0(pth, bc_pth, \"/_1_OISST_baseline_combined.nc\")\noutput_file &lt;- paste0(pth, bc_pth, \"/_2_OISST_climatology.nc\")\n\n# Calculate the mean in each grid cell (across all years in the time period)\ncdo_code &lt;- paste0(\"cdo timmean \", input_file, \" \", output_file )\nsystem(cdo_code)\n\n\nThis results in …\n\nr &lt;- terra::rast(paste0(pth, bc_pth, \"/_2_OISST_climatology.nc\"))\nr #ignore 'time' field\n\nclass       : SpatRaster \ndimensions  : 44, 45, 1  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -126.125, -114.875, 32, 43  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : _2_OISST_climatology.nc:sst \nvarname     : sst (Daily sea surface temperature) \nname        : sst_zlev=0 \nunit        :    Celsius \ntime (days) : 2004-06-30 \n\nplot(r, main = \"OISST climatology 1995-2014\") #smoothed over",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#create-esm-historical-climatology-1995-2014",
    "href": "_5_biascorrection.html#create-esm-historical-climatology-1995-2014",
    "title": "5  Bias correction & downscaling",
    "section": "5.3 Create ESM historical climatology (1995-2014)",
    "text": "5.3 Create ESM historical climatology (1995-2014)\n\n\nCode\nhistclim &lt;- function(model) {\n\n### HISTORICAL \n# This creates a climatology/baseline 1993-2014 file using the ESM's historical run\n# Create baseline file 1993-2014 and mean\nfiley &lt;- list.files(paste0(pth, cmip_pth_proc), \n                    pattern = \"historical\", full.names = T)\nfiley &lt;- filey[grep(model, filey)]\nrr &lt;- rast(filey)\n#time(rr) &lt;- as.Date(time(rr))\nrr &lt;- rr[[time(rr) &gt; as.Date(\"1995-01-01\")]]\nmean_rr &lt;- mean(rr)\nterra::writeCDF(mean_rr, \n                paste0(pth, bc_esm_pth, \"/_1_climatology/tos_mo_\", model, \n                       \"_1995-2014_clim_historical.nc\"),\n                overwrite = T)\n\n}\n\n\nhistclim(\"ACCESS-CM2\")\nhistclim(\"IPSL-CM6A-LR\")\n\n\nThis results in a temperature climatology for each ESM, from 1995-2014.\n\nclim &lt;- terra::rast(paste0(pth, bc_esm_pth, \"/_1_climatology/tos_mo_ACCESS-CM2_1995-2014_clim_historical.nc\"))\nplot(clim, main = \"ACCESS-CM2 climatology 1995-2014\"); maps::map(\"world\", add = T)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#create-anomalies-for-esm-projections-2015-2100",
    "href": "_5_biascorrection.html#create-anomalies-for-esm-projections-2015-2100",
    "title": "5  Bias correction & downscaling",
    "section": "5.4 Create anomalies for ESM projections (2015-2100)",
    "text": "5.4 Create anomalies for ESM projections (2015-2100)\n\n\nCode\nanom_esm &lt;- function(model) {\n  \n  mean_rr &lt;- rast(paste0(pth, bc_esm_pth, \"/_1_climatology/tos_mo_\", model, \n                    \"_1995-2014_clim_historical.nc\"))\n    \n  ### SSPS\n  # Then substract mean climatology from 2015-2100 for both EMSs, creating anomalies\n  filey &lt;- list.files(paste0(pth, cmip_pth_proc), \n                      pattern = \"ssp\", full.names = T)\n  filey &lt;- filey[grep(model, filey)]\n  \n  for (i in 1:length(filey)) {\n    \n    #isolate SSP from string\n    ssp_string &lt;- strsplit(filey[i], model, \"_\")[[1]][2]\n    ssp &lt;- strsplit(ssp_string, \"_r1i1p1f1\")[[1]][1]\n    \n    #import raster and calc anomalies\n    scen &lt;- rast(filey[i])\n    anom &lt;- scen - mean_rr \n    writeCDF(anom, paste0(pth, bc_esm_pth, \"/_2_anomalies/tos_mo_\", \n                          model, \"_2015-2100_anom\", ssp, \".nc\"),\n             overwrite = T)\n  }\n}\n\n# Run function\ntic(); anom_esm(\"ACCESS-CM2\"); toc() #Jessie: 0.689 seconds\nanom_esm(\"IPSL-CM6A-LR\")\n\n\nThe above code results in temperature anomalies for both ESMs (i.e., deviations from the mean). In the image below, the anomalies below represent the ESM projections for ACCESS-CM2 SSP2-4.5, minus the corresponding historical climatology/baseline (1995-2014) created in Step 3.\n\nrr &lt;- terra::rast(paste0(pth, bc_pth, bc_pth_anom, \"/tos_mo_ACCESS-CM2_2015-2100_anom_ssp245.nc\"))\nplot(rr[[1]], main = \"ACCESS-CM2 anomalies SSP245 Jan 2015\"); maps::map(\"world\", add = T)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#create-a-land-mask-with-oisst",
    "href": "_5_biascorrection.html#create-a-land-mask-with-oisst",
    "title": "5  Bias correction & downscaling",
    "section": "5.5 Create a land mask with OISST",
    "text": "5.5 Create a land mask with OISST\nThe CDO code below applies a conditional statement to the sst variable.\n- If sst is equal or greater than 2, sst is replaced with 1.0.\n- If sst is NOT equal or greater than 2, sst is replaced with sst/0.0, which is essentially NaN.\n\n\nCode\ninfile &lt;- paste0(pth, bc_pth, \"/_2_OISST_climatology.nc\")\noutfile &lt;- paste0(pth, bc_pth, \"/_3_OISST_mask.nc\")\n\n# make a mask saying \"NA:land, 1:ocean\"\nsystem(paste0(\"cdo -expr,'sst = ((sst&gt;-2)) ? 1.0 : sst/0.0' \", infile, \" \", outfile))\n\n#Plotting the mask results in...\nr &lt;- rast(outfile)\nplot(r, main = \"OISST land mask\"); values(r) %&gt;% range(na.rm=T); maps::map(\"world\", add = T)\n\n\n\n\n[1] 1 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#interpolate-esm-anomalies-to-oisst-grid",
    "href": "_5_biascorrection.html#interpolate-esm-anomalies-to-oisst-grid",
    "title": "5  Bias correction & downscaling",
    "section": "5.6 Interpolate ESM anomalies to OISST grid",
    "text": "5.6 Interpolate ESM anomalies to OISST grid\nHere, we use bilinear interpolation to remap the course CMIP6 ESM anomalies created in Step 4 to the spatial extent and resolution of OISST, filling in missing cells (i.e., cells close to the coast) with nearest neighbor. Note that this can be processor intensive, especially if working with multiple ESMs and SSPs, so we use parallel processing with furrr.\nThe code below uses a combination of NCO and CDO.\nThe resulting four files are ESM anomalies, that have been remapped and interpolated using OISST.\n\n\nCode\ninput_folder &lt;- paste0(pth, bc_pth, \"/\", bc_pth_anom)\noutput_folder &lt;- paste0(pth, bc_pth, \"/\", bc_pth_anom_rm)\nmsk &lt;- paste0(pth, bc_pth, \"/_3_OISST_mask.nc\")\n\n# Function to do the re-gridding -------------------------------------------\n\ndo_regrid &lt;- function(f) {\n  \n  output_file &lt;- basename(f) %&gt;% # Get input file name\n    gsub(\".nc\", \"_remapped.nc\", .) %&gt;% # Add \"_remapped\" to end of file name\n    paste0(output_folder, \"/\", .) \n  \n  # Step 1 - set grid via nco\n  cdo_code &lt;- paste0('ncatted -O -a units,longitude,c,c,\"degrees_east\" -a units,latitude,c,c,\"degrees_north\" ',\n                     f, \" \", paste0(output_folder, \"/tmp1.nc\"))\n  system(cdo_code)\n  \n  # Get variable name\n  rr &lt;- rast(paste0(output_folder, \"/tmp1.nc\"))\n  vary &lt;- varnames(rr)\n  \n  # Step 2 - remove lat/lon attributes\n  cdo_code2 &lt;- paste0('ncatted -O -a grid_mapping,', vary, ',d,, ', \n                       paste0(output_folder, \"/tmp1.nc\"), ' ', \n                       paste0(output_folder, \"/tmp2.nc\"))\n  system(cdo_code2)\n  \n  # Step 3 - remap ESM anomaly with OISST mask using bilinear interpolation\n  # Set missing vals to nearest neighbour\n  cdo_code3 &lt;- paste0(\"cdo -s -L -f nc4 -z zip \", # Zip the file up\n                      \"-setmisstonn \", # Set missing values to nearest neighbor value\n                      \"-remapbil,\", msk, \" \", # Remap with OISST mask sing bilinear interpolation\n                      paste0(output_folder, \"/tmp2.nc\"), \" \", #input file\n                      paste0(output_folder, \"/tmp3.nc\"))  # output file\n  system(cdo_code3)\n  \n  # Step 4 - remove mask\n  cdo_code &lt;- paste0(\"cdo -s -L -f nc4 -z zip \", # Zip the file up\n                     \"-mul \", msk, \" \", # Multiply the result of the lines, below by the mask to make land NA\n                     paste0(output_folder, \"/tmp3.nc\"), \n                     \" \", output_file) # Mask the remapped anomaly file,  and save as output_file\n  system(cdo_code)\n  \n  # Step 5 -remove temporary files \n  system(paste0(\"rm \", output_folder, \"/tmp1.nc\", \" \", \n                output_folder, \"/tmp2.nc \", \n                output_folder, \"/tmp3.nc\"))\n}\n\n# Run function ------------------------------------------------------------\n\nfiles &lt;- list.files(input_folder, pattern = \"anom\", full.names = TRUE) # Files to process\n plan(multisession, workers = 14) # Setting up to run in parallel, change workers to suit your machine \n tic(); furrr::future_walk(files, do_regrid); toc() \n plan(sequential) # Go back to sequential processing\n \n # If having trouble with using future_walk(), you can try the for loop version.\n # However, this takes longer, since it iterates over each file one at a time!\n # for (f in files) { \n #   do_regrid(f)\n #   print(f)\n # }\n\n\n\nJessie speed: ~6s\n\n\n\n\n\n\n\nThere’s a lot going on in this function. Let’s break it down…\n\n\n\n\n\n\n5.6.1 Create output_file name\nTakes the input file (e.g., tos_mo_ACCESS-CM2_2015-2100_anom_ssp245.nc), and adds _remapped to the end of the file name.\n\noutput_file &lt;- basename(f) %&gt;% # Get input file name\n    gsub(\".nc\", \"_remapped.nc\", .) %&gt;% # Add \"_remapped\" to end of file name\n    paste0(output_folder, \"/\", .) \n\noutput_file\n#[1] \"/Users/admin/Documents/GitHub/BMLworkshop/__data/bias_correct/esm/_3_anomalies_remapped/tos_mo_ACCESS-CM2_2015-2100_anom_ssp245_remapped.nc\"\n\n\n\n5.6.2 Set grid via NCO\nThis takes the file created above, and uses the ncatted command from NCO to modify the input file’s longitude and latitude variables’ attribute units, setting them to degrees_east and degrees_north, respectively. The output is saved as tmp1.nc. We then get the variable name from the file, which will be the original name of the file, e.g., tos_mo_ACCESS-CM2_2015-2100_anom_ssp245.\n\n  cdo_code &lt;- paste0('ncatted -O -a units,longitude,c,c,\"degrees_east\" -a units,latitude,c,c,\"degrees_north\" ',\n                     f, \" \", paste0(output_folder, \"/tmp1.nc\"))\n  system(cdo_code)\n  \n  rr &lt;- rast(paste0(output_folder, \"/tmp1.nc\"))\n  vary &lt;- varnames(rr)\n\n\n\n5.6.3 Remove lon/lat attributes\nThis code removes the grid_mapping attribute from the variable(s) specified in vary, and save the output as tmp2.nc.\n\n  cdo_code2 &lt;- paste0('ncatted -O -a grid_mapping,', vary, ',d,, ', \n                       paste0(output_folder, \"/tmp1.nc\"), ' ', \n                       paste0(output_folder, \"/tmp2.nc\"))\n  system(cdo_code2)\n\n\n\n5.6.4 Remapping\nHere, we remap the ESM anomalies with the OISST mask msk using bilinear interpolation, and set missing values to nearest neighbor. The output is saved as tmp3.nc.\n\n  cdo_code3 &lt;- paste0(\"cdo -s -L -f nc4 -z zip \", # Zip the file up\n                      \"-setmisstonn \", # Set missing values to nearest neighbor value\n                      \"-remapbil,\", msk, \" \", # Remap with OISST mask sing bilinear interpolation\n                      paste0(output_folder, \"/tmp2.nc\"), \" \", #input file\n                      paste0(output_folder, \"/tmp3.nc\"))  # output file\n  system(cdo_code3)\n\n\n\n5.6.5 Remove the OISST mask\nThis code uses the OISST mask to turn land values back to NA.\n\n  cdo_code &lt;- paste0(\"cdo -s -L -f nc4 -z zip \", # Zip the file up\n                     \"-mul \", msk, \" \", # Multiply contents of file with the mask to make land NA\n                     paste0(output_folder, \"/tmp3.nc\"), \n                     \" \", output_file) # Mask the remapped anomaly file,  and save as output_file\n  system(cdo_code)\n\n\n\n5.6.6 Remove temporary files\nWe use the rm command (i.e., a base LINUX function) to remove the three temporary files created previously.\n\n  system(paste0(\"rm \", output_folder, \"/tmp1.nc\", \" \", \n                output_folder, \"/tmp2.nc \", \n                output_folder, \"/tmp3.nc\"))\n\n\n\n5.6.7 Run the function\nHere, we use list.files() to list all files in our input_folder with the pattern anom in the file name. We then set up our session to run in parallel, where I (Jessie) have set to 14 workers (you will need to change this to suit your machine). We then run the do_regrid() function in parallel using future_walk. Once the function has finished running, we set our session back to normal (i.e., sequential) processing.\n\nfiles &lt;- list.files(input_folder, pattern = \"anom\", full.names = TRUE) # Files to process\n plan(multisession, workers = 14) # Setting up to run in parallel, change workers to suit your machine \n tic(); furrr::future_walk(files, do_regrid); toc() #Jessie: 5.7 seconds\n plan(sequential) # Go back to sequential processing\n\n\n\n\n\nBelow left, we see the ESM anomalies created in Step 6, where we subtracted the ESM climatology (1995-2014) from the projections (2015-2100). On the right is the output of this step, representing the remapped anomalies using the OISST mask. Both of these fields represent an ACCESS-CM2 SSP2-4.5 temperature projection for January 2015.\n\n par(mfrow=c(1,2))\n rast(paste0(pth, bc_pth, bc_pth_anom, \"/tos_mo_ACCESS-CM2_2015-2100_anom_ssp245.nc\"))[[1]] %&gt;% \n   plot(main = \"ESM anomalies (proj - hist)\")\n maps::map(\"world\", add =T)\n rast(paste0(pth, bc_pth, bc_pth_anom_rm, \"/tos_mo_ACCESS-CM2_2015-2100_anom_ssp245_remapped.nc\"))[[1]] %&gt;% \n   plot(main = \"Remapped anomalies (OISST mask)\")\n maps::map(\"world\", add =T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting CDO gridding errors\n\n\n\nIf you ever receive errors when using CDO relating to gridding, projections or remapping, it’s a good idea to check the grid structure of the file using griddes, and ensure that the files you are manipulating/trying to create have the same grid structure as your input file.\n\nsystem(paste0(\"cdo griddes \", inputfile))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#bias-correction",
    "href": "_5_biascorrection.html#bias-correction",
    "title": "5  Bias correction & downscaling",
    "section": "5.7 Bias correction",
    "text": "5.7 Bias correction\nNow, the final step: delta-change bias correction. Here, we add the OISST climatology (1995-2014; i.e., the observed mean) to the remapped ESM anomalies created in Step 6. This results in a bias-corrected estimate of ocean surface temperature for each ESM projection. The adjustment to the projection for each time step in the time series amounts to the difference between the means of the observations and ESM outputs over the baseline period – hence the name “delta-change”.\n\n\nCode\ninput_folder &lt;- paste0(pth, bc_esm_pth)\nobs &lt;- paste0(pth, bc_pth, \"/_2_OISST_climatology.nc\")\n\n# Function ----------------------------------------------------------------\n\ndo_add_clim &lt;- function(f) {\n  \n  output_file &lt;- basename(f) %&gt;%  \n    gsub(\"_anom_\", \"_bc_\", .) %&gt;% # Replace anom code in file name with a code for bias corrected (bc)\n    paste0(input_folder, \"/_4_bias_corrected/\", .) # Include the path\n  \n  cdo_code &lt;- paste0(\"cdo -s -L -f nc4 -z zip \", # Zip the file up\n                     \"-add \", f, \" \", # To the remapped regridded anomalies, add...\n                     obs, \" \", output_file) # The observed climatology (map of means)\n  system(cdo_code)\n}\n\n#  Run function ------------------------------------------------------------\n\nfiles &lt;- list.files(paste0(input_folder, \"/_3_anomalies_remapped\"), \n                    pattern = \"remapped\", full.names = TRUE) # The files we want to process\ntic(); walk(files, do_add_clim); toc() \n\n\n\nJessie speed: ~1s\n\nThis results in bias-corrected fields of ocean surface temperature for each month from 2015-2100, for both ESMs and climate scenarios, such as the below field for January 2015 for ACCESS-CM2 SSP-2.45.\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2))\n\ninput_folder &lt;- paste0(pth, \"/__data/bias_correct/esm\")\n\nrr &lt;- rast(paste0(input_folder, \"/_4_bias_corrected/tos_mo_ACCESS-CM2_2015-2100_bc_ssp245_remapped.nc\"))[[1]]\nplot(rr, main = \"Bias corrected w/OISST clim\")\nmaps::map(\"world\", add = T)\n\ntt &lt;- rast(paste0(pth, cmip_pth_proc, \"/tos_Omon_ACCESS-CM2_ssp245_r1i1p1f1_gn_201501-210012.nc\"))[[1]]\nplot(tt, main = \"Raw ESM\")\nmaps::map(\"world\", add = T)\n\nrr &lt;- rast(paste0(pth, bc_esm_pth,  \"/_3_anomalies_remapped/tos_mo_ACCESS-CM2_2015-2100_anom_ssp245_remapped.nc\"))[[1]]  \nplot(rr, col = viridis::magma(255), main = \"Remapped anomalies w/OISST\")\nmaps::map(\"world\", add = T)\n\nrr &lt;- rast(paste0(pth, bc_esm_pth, \"/_2_anomalies/tos_mo_ACCESS-CM2_2015-2100_anom_ssp245.nc\"))[[1]]\nplot(rr, col = viridis::magma(255), main = \"Anomalies Raw ESM\")\nmaps::map(\"world\", add = T)\n\n\n\n\n\n\n\n\n\nAbove: All fields represent January 2015 for ACCESS-CM2 SSP-2.45. Top left: Bias corrected ocean surface temperature (Step 8). Top right: Raw temperature field for the ESM. Bottom left: Remapped anomalies using the OISST grid (Step 6). Bottom right: Anomalies using the raw ESM grid (Step 4).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_5_biascorrection.html#rinse-and-repeat",
    "href": "_5_biascorrection.html#rinse-and-repeat",
    "title": "5  Bias correction & downscaling",
    "section": "5.8 Rinse and repeat",
    "text": "5.8 Rinse and repeat\nNow, we repeat Steps 4-7 for the 1995-2014 time period from both ESMs. This results in bias-corrected monthly temperature fields for this time period, from the historical run.\n\nlist.files(paste0(pth, \"/__data/bias_correct/esm/_4_bias_corrected\"),\n           pattern = \"1995-2014\")\n\n[1] \"tos_mo_ACCESS-CM2_1995-2014_bc_historical_remapped.nc\"  \n[2] \"tos_mo_IPSL-CM6A-LR_1995-2014_bc_historical_remapped.nc\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bias correction & downscaling</span>"
    ]
  },
  {
    "objectID": "_6_taylordiagrams.html",
    "href": "_6_taylordiagrams.html",
    "title": "6  Evaluate accuracy of historical ESM projections",
    "section": "",
    "text": "6.1 Brief explainer\nTaylor Diagrams tell you the degree of pattern correspondence between two datasets (i.e., accuracy). The diagram provides the correlation coefficient and root-mean-square difference between the ESM simulations and observed data, along with the ratio of standard deviations of the two patterns, all in one diagram.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluate accuracy of historical ESM projections</span>"
    ]
  },
  {
    "objectID": "_6_taylordiagrams.html#brief-explainer",
    "href": "_6_taylordiagrams.html#brief-explainer",
    "title": "6  Evaluate accuracy of historical ESM projections",
    "section": "",
    "text": "Correlation coefficient (R) = quantifies pattern similarity. Shown on the plot by the azimuthal position of the simulated field.\n\nRoot-mean-square difference RMS (RMS) = quantifies pattern differences. RMS approaches 0 as patterns are more alike. Shown on plot in the same units as the standard deviation. Proportional to distance apart.\n\nStandard deviation (SD) = variances. Shown on the plot as the radial distance from the origin.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluate accuracy of historical ESM projections</span>"
    ]
  },
  {
    "objectID": "_6_taylordiagrams.html#create-dataframes-of-baseline-climatologies",
    "href": "_6_taylordiagrams.html#create-dataframes-of-baseline-climatologies",
    "title": "6  Evaluate accuracy of historical ESM projections",
    "section": "6.2 Create dataframes of baseline climatologies",
    "text": "6.2 Create dataframes of baseline climatologies\nWe want a data-frame for each product (i.e., OISST raw observations, raw un-bias-corrected ESMs, and the bias-corrected ensemble mean) containing the average SST for each grid cell across 1994-2014.\n\nsource(paste0(pth, \"/__scripts/helpers.R\"))\n\n\n\nCode\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(plotrix)\n\noisst &lt;- rast(paste0(pth, bc_pth, \"/_2_OISST_climatology.nc\"))\naccess_bc &lt;- rast(paste0(pth, bc_pth, bc_pth_bc, \"/tos_mo_ACCESS-CM2_1995-2014_bc_historical_remapped.nc\"))\nipsl_bc &lt;- rast(paste0(pth, bc_pth, bc_pth_bc, \"/tos_mo_IPSL-CM6A-LR_1995-2014_bc_historical_remapped.nc\"))\naccess_raw &lt;- rast(paste0(pth, cmip_pth_proc, \"/tos_Omon_ACCESS-CM2_historical_r1i1p1f1_gn_185001-201412.nc\"))\naccess_raw &lt;- access_raw[[time(access_raw) &gt; \"1994-01-01\"]]\nipsl_raw &lt;- rast(paste0(pth, cmip_pth_proc, \"/tos_Omon_IPSL-CM6A-LR_historical_r1i1p1f1_gn_185001-201412.nc\"))\nipsl_raw &lt;- ipsl_raw[[time(ipsl_raw) &gt; \"1994-01-01\"]]\n\n\n# Convert rasters to dataframes -------------------------------------------\n\n# Calculate mean of each field (i.e., baseline/climatology)\noisst_df &lt;- oisst %&gt;% mean %&gt;% as.data.frame(xy = T)\naccess_df &lt;- access_raw %&gt;% mean %&gt;% as.data.frame(xy = T)\naccess_bc_df &lt;- access_bc %&gt;% mean %&gt;% as.data.frame(xy = T)\nipsl_df &lt;- ipsl_raw %&gt;% mean %&gt;% as.data.frame(xy = T)\nipsl_bc_df &lt;- ipsl_bc %&gt;% mean %&gt;% as.data.frame(xy = T)\n\n\n# Merge and fix names -----------------------------------------------------\n\nalldata &lt;- inner_join(oisst_df, access_df, by = c(\"x\",\"y\")) \nnames(alldata) &lt;- c(\"x\", \"y\", \"oisst_mean\", \"access_mean\")\nalldata &lt;- inner_join(alldata, access_bc_df, by = c(\"x\",\"y\"))\nnames(alldata) &lt;- c(\"x\", \"y\", \"oisst_mean\", \"access_mean\", \"access_bc_mean\")\n\nalldata2 &lt;- inner_join(oisst_df, ipsl_df, by = c(\"x\",\"y\")) \nnames(alldata2) &lt;- c(\"x\", \"y\", \"oisst_mean\", \"ipsl_mean\")\nalldata2 &lt;- inner_join(alldata2, ipsl_bc_df, by = c(\"x\",\"y\"))\nnames(alldata2) &lt;- c(\"x\", \"y\", \"oisst_mean\", \"ipsl_mean\", \"ipsl_bc_mean\")\n\nalldata &lt;- inner_join(alldata, alldata2)\n\nalldata &lt;- alldata %&gt;% #ensemble mean\n  mutate(ens_bc_mean = (access_bc_mean + ipsl_bc_mean) / 2)\n\nalldata %&gt;% head\n\n\n        x      y oisst_mean access_mean access_bc_mean ipsl_mean ipsl_bc_mean\n1 -126.00 42.875   12.65070    13.22723       12.65070  13.64941     12.65070\n2 -125.75 42.875   12.54835    13.24877       12.54835  13.54898     12.54835\n3 -125.50 42.875   12.39874    13.27030       12.39874  13.44833     12.39874\n4 -126.00 42.625   12.66040    13.22705       12.66040  13.59311     12.66040\n5 -125.75 42.625   12.54365    13.24496       12.54365  13.49520     12.54365\n6 -125.50 42.625   12.37454    13.26286       12.37454  13.39598     12.37454\n  ens_bc_mean\n1    12.65070\n2    12.54835\n3    12.39874\n4    12.66040\n5    12.54365\n6    12.37454",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluate accuracy of historical ESM projections</span>"
    ]
  },
  {
    "objectID": "_6_taylordiagrams.html#construct-taylor-diagram",
    "href": "_6_taylordiagrams.html#construct-taylor-diagram",
    "title": "6  Evaluate accuracy of historical ESM projections",
    "section": "6.3 Construct Taylor Diagram",
    "text": "6.3 Construct Taylor Diagram\nThen, we plot our Taylor Diagram. Our OISST observations (i.e., reality) is the red circle. If our models perfectly represent reality, they should all plot on top of this circle. In this case, the raw ACCESS-CM2 historical run does OK at simulating reality, with a correlation of ~80%. IPSL does slightly worse at around 75%. However, the bias-corrected ensemble average of the two models (i.e., black triangle) plots directly on top of our observations, with almost a 1:1 correlation, meaning it does really well at simulating reality, at least for 1995-2014! Further, the bias-corrected ensemble mean has an improved RMSE (i.e., close to 0) and normalized standard deviation (i.e., 1).\nThis diagram demonstrates the sheer value of bias-correcting ESM outputs, before making projections.\n\n\nCode\ntaylor.diagram(alldata$oisst_mean, \n               alldata$oisst_mean, \n               ref.sd = T, #display arc of ref. std. dev. (i.e., 1)\n               normalize=TRUE, #normalize models so ref has SD of 1\n               sd.arcs=TRUE,  #display arcs along SD axes\n               pcex = 4,\n               pch = 19,\n               col = \"red\",\n               xlab = \"Standard deviation (normalised)\",\n               pos.cor = T, #show correlation (y-axis) from 0-1 \n               gamma.col = \"blue\", #RMSE arcs\n               main=\"OISST vs. CMIP6 ESM tos (SST) 1995-2014\")\n\n# Add ESM points\ntaylor.diagram(alldata$oisst_mean,\n               alldata$access_mean,\n               add=TRUE, normalize=TRUE,  \n               pcex=3, pch=17, col= \"purple\")\n\ntaylor.diagram(alldata$oisst_mean,\n               alldata$ipsl_mean,\n               add=TRUE, normalize=TRUE,  \n               pcex=3, pch=17, col= \"forestgreen\")\n\n# Add bias-corrected point\ntaylor.diagram(alldata$oisst_mean,\n               alldata$access_bc_mean,\n               add=TRUE, normalize=TRUE,  \n               pcex=2, pch=17, col= \"black\")\n\n\n# Legend ------------------------------------------------------------------\n\nlegend(1.2, 1.7, cex=1, pt.cex=2, pch=17,\n       legend=c(\"ACCESS-CM2\", \"IPSL-CM6A-LR\"),\n       col=c(\"purple\", \"forestgreen\"), \n       bty = \"n\")\n\nlegend(1.2, 1.54, cex=1, pt.cex=2, pch=19,\n       legend=c(\"OISST\"),\n       col= 'red', \n       bty = \"n\")\nlegend(1.2, 1.45, cex=1, pt.cex=2, pch=17,\n       legend=c(\"Bias-corrected ESMs\"),\n       col= 'black', \n       bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\nTaylor, Karl E. 2001. “Summarizing Multiple Aspects of Model Performance in a Single Diagram.” Journal of Geophysical Research: Atmospheres 106 (D7): 7183–92. https://doi.org/https://doi.org/10.1029/2000JD900719.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Evaluate accuracy of historical ESM projections</span>"
    ]
  },
  {
    "objectID": "_7_projections.html",
    "href": "_7_projections.html",
    "title": "7  Making projections",
    "section": "",
    "text": "7.1 Time series\nWe are going to create two time series of temperature from 2015 to 2100, for the entire California Current region - one for each SSP.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making projections</span>"
    ]
  },
  {
    "objectID": "_7_projections.html#time-series",
    "href": "_7_projections.html#time-series",
    "title": "7  Making projections",
    "section": "",
    "text": "7.1.1 Create yearly averages of temperature\nBecause we are interested in the long-term trend of temperature out to 2100, we are going to average our temperature values across each year, and, apply a five-year moving window to smooth out any year-to-year variability.\n\n\nCode\nmodels = c(\"ACCESS-CM2|IPSL-CM6A-LR\")\nmodels2 = c(\"ACCESS-CM2\", \"IPSL-CM6A-LR\")\n\nindir_proj &lt;- paste0(pth, \"/__data/bias_correct/esm/_4_bias_corrected\")\noutdir &lt;- \"__data/timeseries\"\n\n# Projections -------------------------------------------------------------\n\ntimeseries_ssp &lt;- function(ssp) {\n  \n  allfiles_proj &lt;- list.files(indir_proj, pattern = models, full.names = T)\n  allfiles_proj2 &lt;- allfiles_proj[grep(ssp, allfiles_proj)]\n  rr &lt;- rast(allfiles_proj2)\n  rr &lt;- rr[[time(rr) &lt; \"2100-12-31\"]] #Only data until 2100\n  dateys &lt;- time(rr) %&gt;% unique\n  years &lt;- lubridate::year(dateys) %&gt;% unique #get years\n\n  emplist &lt;- list()\n  emplist_allmodels &lt;- list()\n  \n  ### Ensemble mean\n  for (i in 1:length(years)) {\n    \n    alldates &lt;- dateys[grep(years[i], dateys)]\n    rasty &lt;- rr[[time(rr) == alldates]]\n    meanrast &lt;- mean(rasty) # Average across the year\n    meanval &lt;- values(meanrast) %&gt;% mean(na.rm=T) #Get average temp for California \n    forlist &lt;- data.frame(date = years[i], value = meanval) \n    emplist[[i]] &lt;- forlist\n    print(paste0(\"ens_\", years[i]))\n  }\n  \n  ### Individual models\n  for (j in 1:length(models2)) {\n    emplist_indmodel &lt;- list()\n\n    for (h in 1:(length(years)-1)) {\n      modelrast &lt;- rast(allfiles_proj2[grep(models2[j], allfiles_proj2)])\n      \n      dateys &lt;- time(modelrast) %&gt;% unique\n      alldates &lt;- dateys[grep(years[h], dateys)]\n      modelrast &lt;- modelrast[[time(modelrast) == alldates]]\n      meanrast &lt;- mean(modelrast) \n      meanval &lt;- values(meanrast) %&gt;% mean(na.rm=T)\n      forlist &lt;- data.frame(model = models2[j], date = years[h], value = meanval)\n      emplist_indmodel[[h]] &lt;- forlist\n    }\n    \n    toadd &lt;- do.call(rbind, emplist_indmodel)\n    emplist_allmodels[[j]] &lt;- toadd\n    print(models2[j])\n  }\n  \n  saveRDS(emplist, \n          paste0(\"__data/timeseries/sst_year_proj_ens_\",\n                 ssp, \".RDS\"))\n  saveRDS(emplist_allmodels,\n          paste0(\"__data/timeseries/sst_year_proj_ind_\",\n                 ssp, \".RDS\"))\n}\n\nssps &lt;- c(\"ssp245\", \"ssp585\")\ntic(); future_walk(ssps, timeseries_ssp); toc() #26 seconds for both\n\n# For loop version\n#tic(); timeseries_ssp(\"ssp245\"); toc() #Jessie: 16.32 seconds for one\n#timeseries_ssp(\"ssp585\")\n\n\n\nJessie speed: ~26s\n\nThis results in two dataframes for each SSP: (i) one containing the ensemble mean of temperature for each year, and (ii) one containing the mean temperature across each model. For example…\n\n# Ensemble mean\nens &lt;- readRDS(\"__data/timeseries/sst_year_proj_ens_ssp245.RDS\")\nens &lt;- do.call (rbind, ens)\nhead(ens)\n\n  date    value\n1 2015 14.43567\n2 2016 14.81477\n3 2017 14.88652\n4 2018 14.72109\n5 2019 14.67123\n6 2020 14.95188\n\n\n\n# For each ESM...\nind &lt;- readRDS(\"__data/timeseries/sst_year_proj_ind_ssp245.RDS\")\nind &lt;- do.call (rbind, ind)\nhead(ind)\n\n       model date    value\n1 ACCESS-CM2 2015 14.89644\n2 ACCESS-CM2 2016 15.33568\n3 ACCESS-CM2 2017 14.99773\n4 ACCESS-CM2 2018 14.78380\n5 ACCESS-CM2 2019 14.46701\n6 ACCESS-CM2 2020 15.16607\n\n\n\n\n\n\n\n\nCaution\n\n\n\nToday, we’re using the ensemble mean. But in your own work, you may want to consider using the ensemble median instead. The median may be less impacted by outliers/extremes in the data, especially if you’re using ESMs that are known to be ‘too-hot’ in your region of interest.\n\n\n\n\n7.1.2 Bind everything and apply 5-yr smooth\nHere, we apply a 5 year smooth to our time series via the window_size argument in our smooth_esms() function. We do this to smooth out some inter-annual variability, as we’re interested in long-term trends.\n\n\nCode\nsmooth_esms &lt;- function(ssp, window_size) {\n\n# Ensemble mean  \ntimeseries_proj_ens &lt;- readRDS(paste0(\"__data/timeseries/sst_year_proj_ens_\", ssp, \".RDS\"))\n\n  ens &lt;- do.call(rbind, timeseries_proj_ens)\n  zoo_data &lt;- zoo(ens$value, order.by = ens$date)\n  smoothed_esm &lt;- rollapply(zoo_data, width = window_size, \n                            FUN = mean, \n                            align = \"center\", \n                            fill = \"extend\")\n  smooth_esm &lt;- data.frame(date = time(smoothed_esm), \n                              values = coredata(smoothed_esm))\n  assign(paste0(\"smooth_esm_\", ssp), \n         smooth_esm, \n         envir = globalenv())\n  \n# Individual models\n  timeseries_proj &lt;- readRDS(paste0(\"__data/timeseries/sst_year_proj_ind_\", ssp, \".RDS\"))\n  timeseries_allmodels &lt;- do.call(rbind, timeseries_proj)\n  allmodels &lt;- timeseries_allmodels\n  emplist &lt;- list()\n  ens2 &lt;- allmodels\n\n  for (i in 1:length(unique(allmodels$model))) {\n    \n    ens &lt;- subset(ens2, model == unique(ens2$model)[i])\n    zoo_data &lt;- zoo(ens$value, order.by = ens$date)\n    smoothed_esm &lt;- rollapply(zoo_data, width = window_size, \n                              FUN = mean, align = \"center\", fill = \"extend\")\n    smooth_11_esm &lt;- data.frame(date = time(smoothed_esm), \n                                values = coredata(smoothed_esm))\n    smooth_11_esm$model &lt;- unique(ens$model)\n    emplist[[i]] &lt;- smooth_11_esm\n  }\n\n  ssp_smoothed_ind &lt;- do.call(rbind, emplist)\n  assign(paste0(\"ssp_smoothed_ind_\", ssp), \n         ssp_smoothed_ind, \n         envir = globalenv())\n  \n}\n\ntic(); smooth_esms(\"ssp245\", window_size = 5); toc() \nsmooth_esms(\"ssp585\", window_size = 5)\n\n\n\nJessie speed: ~0.01s\n\nThis creates four objects in our global environment, which we will use to plot our time series: ssp_smoothed_ind_ssp245, ssp_smoothed_ind_ssp585, smooth_esm_ssp245 and smooth_esm_ssp585.\n\n\n7.1.3 Plot!\nFinally, the fun part.\n\n\nCode\nplot_ts &lt;- function(ssp, sspletter) {\n\n  smooth_esm &lt;- get(paste0(\"smooth_esm_\", ssp)) #relies on this being in global env\n  ssp_smoothed_ind &lt;- get(paste0(\"ssp_smoothed_ind_\", ssp))\n  \n  # Make a kick-ass plot\n p1 &lt;- ggplot() +\n    geom_line(smooth_esm, \n              mapping = aes(x = date, y = values), \n              lwd = 1.5) +\n  geom_rect(data = data.frame(), \n            mapping = aes(xmin = 2020, xmax = 2040, ymin = -Inf, ymax = Inf),\n             fill = \"grey\",\n            alpha = 0.4) +\n   geom_rect(data = data.frame(), \n             mapping = aes(xmin = 2080, xmax = 2100, ymin = -Inf, ymax = Inf),\n             fill = \"grey\", \n             alpha = 0.4) + \n   geom_line(smooth_esm, \n             mapping = aes(x = date, y = values), \n             lwd = 1.5) +\n geom_line(subset(ssp_smoothed_ind, model == \"ACCESS-CM2\"), \n              mapping = aes(x = date, y = values), \n           col = \"black\", \n           alpha = 0.3) +\n    geom_line(subset(ssp_smoothed_ind, model == \"IPSL-CM6A-LR\"), \n              mapping = aes(x = date, y = values), \n              col = \"black\", \n              alpha = 0.3) +\n    theme_bw() + \n    scale_x_continuous(name = \"Year\", \n                       n.breaks = 6) +\n    scale_y_continuous(name = \"SST (˚C)\", \n                       limits = c(13.5, 20.5)) +\n    theme(panel.grid.minor = element_blank(),\n          plot.margin=unit(c(1,0.1,.1,0.1),\"cm\"),\n          axis.title = element_text(size = 20, \n                                    family = \"Arial Narrow\",\n                                    face = \"bold\"),\n          axis.text = element_text(size = 20, \n                                   family = \"Arial Narrow\"),\n          axis.title.x = element_text(margin = margin(t = 10, r = -20))) +\n   annotate(\"text\", x = 2019, y = 20.1,\n            label = sspletter, \n            size = 9, \n            fontface = \"bold\", \n            family = \"Arial Narrow\", \n            hjust = 0, \n            vjust = 1) +\n   annotate(\"text\", x = 2083, y = 13.7,\n            label = \"Long-term\", size = 5, \n            family = \"Arial Narrow\", \n            hjust = 0, \n            vjust = 1) + \n   annotate(\"text\", x = 2023, y = 13.7,\n            label = \"Short-term\", size = 5, \n            family = \"Arial Narrow\", \n            hjust = 0, \n            vjust = 1)\n \n     ggsave(p1, \n           filename = paste0(outdir, \"/\", \n                             ssp, \"_SST_timeseries_1995-2100_11yrsmooth.png\"),\n           width = 8, height = 5)\n  \n}\n\n# Run function\ntic(); plot_ts(\"ssp245\", sspletter = \"SSP2-4.5\"); toc() #Jessie: 0.197 seconds\nplot_ts(\"ssp585\", sspletter = \"SSP5-8.5\")\n\n\nThis results in two plots: a time series of projected yearly averaged temperature from 2015-2100, for each climate scenario, for the entire California Current ecosystem. The bold black line represents the ensemble mean of the two ESMs, and the two grey lines represent the yearly means for each ESM, respectively, giving us an idea of potential model uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making projections</span>"
    ]
  },
  {
    "objectID": "_7_projections.html#projections",
    "href": "_7_projections.html#projections",
    "title": "7  Making projections",
    "section": "7.2 Projections",
    "text": "7.2 Projections\nLet’s make some maps.\n\n7.2.1 Individual ESM projections\nFirst, we create our projections for both ESMs, across the two time periods and SSPs, and save the outputs as .nc files. Our projections consist of SST averaged across each month within each time period, plus, the associated standard deviation of SST.\n\n\nCode\nmodels = c(\"ACCESS-CM2\", \"IPSL-CM6A-LR\")\nssps = c(\"ssp245\", \"ssp585\")\nterm = c(\"near\", \"mid\", \"long\")\noutdir = paste0(pth, \"/__data/projections\")\nindir_proj &lt;- paste0(pth, bc_pth, bc_pth_bc)\n\ntermdf &lt;- data.frame(timeperiod = c(\"near\", \"long\"),\n                     st = c(\"2020-01-01\", \"2080-01-01\"),\n                     fin = c(\"2040-01-01\", \"2100-01-01\"))\n\ntic(); for (k in term) {\n  for (i in ssps) {\n    for (j in models) {\n      \n      allfiles_proj &lt;- list.files(indir_proj, pattern = j, full.names = T)\n      allfiles_proj &lt;- allfiles_proj[grep(i, allfiles_proj)]\n      allfiles_proj &lt;- allfiles_proj[grep(\"2100\", allfiles_proj)]\n      rr &lt;- rast(allfiles_proj)\n      \n      # subset to time period\n      tp &lt;- subset(termdf, timeperiod == k)\n      rr &lt;- rr[[time(rr) &gt; tp[,\"st\"] & time(rr) &lt; tp[,\"fin\"] ]]\n      \n      # Mean and SD\n      proj_u &lt;- mean(rr)\n      proj_sd &lt;- stdev(rr)\n      # write to outdir\n      filename_u &lt;- paste0(outdir, \"/ind/mean_\", j, \"_\", i, \"_\", k, \"_\", \"proj.nc\" )\n      filename_sd &lt;- paste0(outdir, \"/ind/sd_\", j, \"_\", i, \"_\", k, \"_\", \"proj.nc\" )\n      terra::writeCDF(proj_u, filename_u, overwrite = T)\n      terra::writeCDF(proj_sd, filename_sd, overwrite = T)\n    }\n  }\n}; toc() #Jessie: 1.8 seconds\n\n\n\nJessie speed: ~2s\n\nThis results in…\n\nlist.files(paste0(pth, \"/__data/projections/ind\"))\n\n [1] \"mean_ACCESS-CM2_ssp245_long_proj.nc\"  \n [2] \"mean_ACCESS-CM2_ssp245_near_proj.nc\"  \n [3] \"mean_ACCESS-CM2_ssp585_long_proj.nc\"  \n [4] \"mean_ACCESS-CM2_ssp585_near_proj.nc\"  \n [5] \"mean_IPSL-CM6A-LR_ssp245_long_proj.nc\"\n [6] \"mean_IPSL-CM6A-LR_ssp245_near_proj.nc\"\n [7] \"mean_IPSL-CM6A-LR_ssp585_long_proj.nc\"\n [8] \"mean_IPSL-CM6A-LR_ssp585_near_proj.nc\"\n [9] \"sd_ACCESS-CM2_ssp245_long_proj.nc\"    \n[10] \"sd_ACCESS-CM2_ssp245_near_proj.nc\"    \n[11] \"sd_ACCESS-CM2_ssp585_long_proj.nc\"    \n[12] \"sd_ACCESS-CM2_ssp585_near_proj.nc\"    \n[13] \"sd_IPSL-CM6A-LR_ssp245_long_proj.nc\"  \n[14] \"sd_IPSL-CM6A-LR_ssp245_near_proj.nc\"  \n[15] \"sd_IPSL-CM6A-LR_ssp585_long_proj.nc\"  \n[16] \"sd_IPSL-CM6A-LR_ssp585_near_proj.nc\"  \n\n\n\n\n7.2.2 Ensembled projections\nNow, we can average over the individual ESMs to create our ensembled projections.\n\n\nCode\ntic(); for (k in term) {\n  for (i in ssps) {\n    \n      \n      allfiles_proj &lt;- list.files(paste0(pth, \"/__data/projections/ind\"), \n                                  pattern = i, full.names = T)\n      allfiles_proj &lt;- allfiles_proj[grep(k, allfiles_proj)]\n      allfiles_proj &lt;- allfiles_proj[grep(\"mean\", allfiles_proj)]\n      rr &lt;- rast(allfiles_proj)\n      \n      # Mean and SD\n      proj_u &lt;- mean(rr)\n      proj_sd &lt;- stdev(rr)\n    \n      # write to outdir\n      filename_u &lt;- paste0(outdir, \"/ens/mean_ens_\", i, \"_\", k, \"_\", \"proj.nc\" )\n      filename_sd &lt;- paste0(outdir, \"/ens/sd_ens_\", i, \"_\", k, \"_\", \"proj.nc\" )\n      terra::writeCDF(proj_u, filename_u, overwrite = T)\n      terra::writeCDF(proj_sd, filename_sd, overwrite = T)\n      \n  }\n}; toc() #Jessie: 0.3 seconds\n\n\n\nJessie speed: ~0.3s\n\n\nlist.files(paste0(pth, \"/__data/projections/ens\"))\n\n[1] \"mean_ens_ssp245_long_proj.nc\" \"mean_ens_ssp245_near_proj.nc\"\n[3] \"mean_ens_ssp585_long_proj.nc\" \"mean_ens_ssp585_near_proj.nc\"\n[5] \"sd_ens_ssp245_long_proj.nc\"   \"sd_ens_ssp245_near_proj.nc\"  \n[7] \"sd_ens_ssp585_long_proj.nc\"   \"sd_ens_ssp585_near_proj.nc\"  \n\n\n\n\n7.2.3 Delta difference\nNow that we have our ensembled projections of SST, we want to inspect the delta difference, which is the difference between projected SSTs, and baseline/observed SST experienced between 1995-2014. Essentially, how much will SST change under projected future ocean conditions?\nFirst, we create our historical average, from 1995-2014:\n\n\nCode\n#Ensemble average of 1995-2014 for both models\nbc_pth &lt;- paste0(pth, \"/__data/bias_correct/esm/_4_bias_corrected/\")\nr1 &lt;- rast(paste0(bc_pth, \"tos_mo_ACCESS-CM2_1995-2014_bc_historical_remapped.nc\"))\nr2 &lt;- rast(paste0(bc_pth, \"tos_mo_IPSL-CM6A-LR_1995-2014_bc_historical_remapped.nc\"))\nrr &lt;- c(r1, r2)\nmean_hist &lt;- mean(rr)\nplot(mean_hist, main = \"Ensembled SST 1995-2014\"); maps::map(\"world\", add = T)\n\n\n\n\n\n\n\n\n\nThen, we subtract the baseline SST from our projections…\n\n\nCode\nssps = c(\"ssp245\", \"ssp585\")\n\ntic(); for (i in ssps) {\n  \n  near_mean &lt;- rast(paste0(pth, \"/__data/projections/ens/mean_ens_\", i, \"_near_proj.nc\"))\n  rr &lt;- near_mean - mean_hist\n  writeCDF(rr, paste0(outdir, \"/delta_mean_ens_near_\", i, \".nc\"),\n           overwrite = T)\n  \n  mid_mean &lt;- rast(paste0(pth, \"/__data/projections/ens/mean_ens_\", i, \"_mid_proj.nc\"))\n  rr &lt;- mid_mean - mean_hist\n  writeCDF(rr, paste0(outdir, \"/delta_mean_ens_mid_\", i, \".nc\"),\n           overwrite = T)\n  \n  long_mean &lt;- rast(paste0(pth, \"/__data/projections/ens/mean_ens_\", i, \"_long_proj.nc\"))\n  rr &lt;- long_mean - mean_hist\n  writeCDF(rr, paste0(outdir, \"/delta_mean_ens_long_\", i, \".nc\"),\n           overwrite = T)\n  \n}; toc() \n\n\n\nJessie speed: ~0.3s\n\n\nlist.files(paste0(pth, \"/__data/projections/delta\"))\n\n[1] \"delta_mean_ens_long_ssp245.nc\" \"delta_mean_ens_long_ssp585.nc\"\n[3] \"delta_mean_ens_near_ssp245.nc\" \"delta_mean_ens_near_ssp585.nc\"\n\n\n\n\n7.2.4 Plot: SSP2-4.5\nNow, we plot. Top row = near-term projections (2020-2040) for SST (left), standard deviation of SST (mid) and delta difference (right). Bottom row = long-term projections (2080-2100).\n\n\n\n\n\n\n\n\n\n\n\n7.2.5 Plot: SSP5-8.5\n…And repeat for SSP5-8.5. In this scenario, we can see the ocean off California is projected to increase by ~1˚C in the near-term, and ~4.5˚C in the long-term, compared to baseline SST (note the increased uncertainty, particularly in SoCal).\n\n\n\n\n\n\n\n\n\nWoohoo! We’ve made SST projections for the California region using an ensemble of two ESMs, across two time periods, and, for two climate scenarios. Now, we could use these projections for species distribution modelling, marine disease projections… the list is endless! Some useful summary papers that have used ESMs to assess the impact of climate on marine fauna include (Stock et al. 2011) and (Drenkard et al. 2021).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making projections</span>"
    ]
  },
  {
    "objectID": "_7_projections.html#uncertainty",
    "href": "_7_projections.html#uncertainty",
    "title": "7  Making projections",
    "section": "7.3 Uncertainty",
    "text": "7.3 Uncertainty\nWe could dedicate an entire workshop to reviewing the different sources of uncertainty when working with ESMs, but we don’t have time!\nLinked here are a few fantastic papers that delve into this topic in more detail: (Brodie et al. 2022), (Morley, Frölicher, and Pinsky 2020), (Cheung et al. 2016) and (Thuiller et al. 2019).\n\n\n\n\nBrodie, Stephanie, James A. Smith, Barbara A. Muhling, Lewis A. K. Barnett, Gemma Carroll, Paul Fiedler, Steven J. Bograd, et al. 2022. “Recommendations for Quantifying and Reducing Uncertainty in Climate Projections of Species Distributions.” Global Change Biology 28 (22): 6586–6601. https://doi.org/https://doi.org/10.1111/gcb.16371.\n\n\nCheung, William W. L., Thomas L. Frölicher, Rebecca G. Asch, Miranda C. Jones, Malin L. Pinsky, Gabriel Reygondeau, Keith B. Rodgers, et al. 2016. “Building Confidence in Projections of the Responses of Living Marine Resources to Climate Change.” ICES Journal of Marine Science 73 (5): 1283–96. https://doi.org/10.1093/icesjms/fsv250.\n\n\nDrenkard, Elizabeth J, Charles Stock, Andrew C Ross, Keith W Dixon, Alistair Adcroft, Michael Alexander, Venkatramani Balaji, et al. 2021. “Next-Generation Regional Ocean Projections for Living Marine Resource Management in a Changing Climate.” ICES Journal of Marine Science 78 (6): 1969–87. https://doi.org/10.1093/icesjms/fsab100.\n\n\nMorley, James W, Thomas L Frölicher, and Malin L Pinsky. 2020. “Characterizing Uncertainty in Climate Impact Projections: A Case Study with Seven Marine Species on the North American Continental Shelf.” ICES Journal of Marine Science 77 (6): 2118–33. https://doi.org/10.1093/icesjms/fsaa103.\n\n\nStock, Charles A., Michael A. Alexander, Nicholas A. Bond, Keith M. Brander, William W. L. Cheung, Enrique N. Curchitser, Thomas L. Delworth, et al. 2011. “On the Use of IPCC-Class Models to Assess the Impact of Climate on Living Marine Resources.” Progress in Oceanography 88 (1): 1–27. https://doi.org/https://doi.org/10.1016/j.pocean.2010.09.001.\n\n\nThuiller, Wilfried, Maya Guéguen, Julien Renaud, Dirk N. Karger, and Niklaus E. Zimmermann. 2019. “Uncertainty in Ensembles of Global Biodiversity Scenarios.” Nature Communications 10 (1): 1446. https://doi.org/10.1038/s41467-019-09519-w.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making projections</span>"
    ]
  },
  {
    "objectID": "_xxxxx_handyresources.html",
    "href": "_xxxxx_handyresources.html",
    "title": "8  Handy resources",
    "section": "",
    "text": "CDO reference card, listing common operators\nhotrstuff, an upcoming R package that’s currently in development (as of December 2024) to facilitate the rapid download, wrangling and processing of ESM output from CMIP. Keep an eye on this - a lot of the functions in this package correspond to tasks we learned today!\nclimate4R, a bundle of R packages for transparent climate data access, post-processing (including data collocation and bias correction / downscaling) and visualization.\nA great explainer from CarbonBrief on how climate models work, and another explainer on CMIP6 more broadly.\nA good news article on the limitations of climate modelling efforts.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Handy resources</span>"
    ]
  },
  {
    "objectID": "_xxxxxx_references.html",
    "href": "_xxxxxx_references.html",
    "title": "References",
    "section": "",
    "text": "Bi, Daohua, Martin Dix, Simon Marsland, Siobhan O’Farrell, Arnold\nSullivan, Roger Bodman, Rachel Law, et al. 2020. “Configuration\nand Spin-up of ACCESS-CM2, the New Generation\nAustralian Community Climate and\nEarth System Simulator\nCoupled Model.” Journal of Southern\nHemisphere Earth Systems Science 70 (1): 225–51. https://doi.org/10.1071/ES19040.\n\n\nBoucher, Olivier, Jérôme Servonnat, Anna Lea Albright, Olivier Aumont,\nYves Balkanski, Vladislav Bastrikov, Slimane Bekki, et al. 2020.\n“Presentation and Evaluation of the IPSL-CM6A-LR Climate\nModel.” Journal of Advances in Modeling Earth Systems 12\n(7): e2019MS002010. https://doi.org/https://doi.org/10.1029/2019MS002010.\n\n\nBrodie, Stephanie, James A. Smith, Barbara A. Muhling, Lewis A. K.\nBarnett, Gemma Carroll, Paul Fiedler, Steven J. Bograd, et al. 2022.\n“Recommendations for Quantifying and Reducing Uncertainty in\nClimate Projections of Species Distributions.” Global Change\nBiology 28 (22): 6586–6601. https://doi.org/https://doi.org/10.1111/gcb.16371.\n\n\nCheung, William W. L., Thomas L. Frölicher, Rebecca G. Asch, Miranda C.\nJones, Malin L. Pinsky, Gabriel Reygondeau, Keith B. Rodgers, et al.\n2016. “Building Confidence in Projections of the Responses of\nLiving Marine Resources to Climate Change.” ICES Journal of\nMarine Science 73 (5): 1283–96. https://doi.org/10.1093/icesjms/fsv250.\n\n\nDrenkard, Elizabeth J, Charles Stock, Andrew C Ross, Keith W Dixon,\nAlistair Adcroft, Michael Alexander, Venkatramani Balaji, et al. 2021.\n“Next-Generation Regional Ocean Projections for Living Marine\nResource Management in a Changing Climate.” ICES Journal of\nMarine Science 78 (6): 1969–87. https://doi.org/10.1093/icesjms/fsab100.\n\n\nMorley, James W, Thomas L Frölicher, and Malin L Pinsky. 2020.\n“Characterizing Uncertainty in Climate Impact Projections: A Case\nStudy with Seven Marine Species on the North American Continental\nShelf.” ICES Journal of Marine Science 77 (6): 2118–33.\nhttps://doi.org/10.1093/icesjms/fsaa103.\n\n\nSchoeman, David S., Alex Sen Gupta, Cheryl S. Harrison, Jason D.\nEverett, Isaac Brito-Morales, Lee Hannah, Laurent Bopp, Patrick R.\nRoehrdanz, and Anthony J. Richardson. 2023. “Demystifying Global\nClimate Models for Use in the Life Sciences.” Trends in\nEcology & Evolution 38 (9): 843–58. https://doi.org/https://doi.org/10.1016/j.tree.2023.04.005.\n\n\nStock, Charles A., Michael A. Alexander, Nicholas A. Bond, Keith M.\nBrander, William W. L. Cheung, Enrique N. Curchitser, Thomas L.\nDelworth, et al. 2011. “On the Use of IPCC-Class Models to Assess\nthe Impact of Climate on Living Marine Resources.” Progress\nin Oceanography 88 (1): 1–27. https://doi.org/https://doi.org/10.1016/j.pocean.2010.09.001.\n\n\nTaylor, Karl E. 2001. “Summarizing Multiple Aspects of Model\nPerformance in a Single Diagram.” Journal of Geophysical\nResearch: Atmospheres 106 (D7): 7183–92. https://doi.org/https://doi.org/10.1029/2000JD900719.\n\n\nThuiller, Wilfried, Maya Guéguen, Julien Renaud, Dirk N. Karger, and\nNiklaus E. Zimmermann. 2019. “Uncertainty in Ensembles of Global\nBiodiversity Scenarios.” Nature Communications 10 (1):\n1446. https://doi.org/10.1038/s41467-019-09519-w.",
    "crumbs": [
      "References"
    ]
  }
]